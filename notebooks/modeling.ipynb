{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.ipynb\n",
    "\n",
    "This notebook will handle modeling and data preprocessing for our problem.\n",
    "\n",
    "Evaluation Metrics:\n",
    "- Brier Score (this is the main one)\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import brier_score_loss, accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>lower_TeamID</th>\n",
       "      <th>lower_Wins</th>\n",
       "      <th>lower_Losses</th>\n",
       "      <th>lower_Winning Percentage</th>\n",
       "      <th>lower_Score_mean</th>\n",
       "      <th>lower_FGM_mean</th>\n",
       "      <th>lower_FGA_mean</th>\n",
       "      <th>lower_FGM3_mean</th>\n",
       "      <th>lower_FGA3_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>higher_AssistToTurnoverRatio_std</th>\n",
       "      <th>higher_Possessions_std</th>\n",
       "      <th>higher_OffEff_std</th>\n",
       "      <th>higher_DefEff_std</th>\n",
       "      <th>higher_TO%_std</th>\n",
       "      <th>higher_PointDiff_std</th>\n",
       "      <th>higher_OffensiveRating_std</th>\n",
       "      <th>higher_DefensiveRating_std</th>\n",
       "      <th>Bracket</th>\n",
       "      <th>LowerWin?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>2019</td>\n",
       "      <td>1242</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>75.382353</td>\n",
       "      <td>27.294118</td>\n",
       "      <td>59.058824</td>\n",
       "      <td>7.235294</td>\n",
       "      <td>20.647059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881175</td>\n",
       "      <td>7.485740</td>\n",
       "      <td>25.135399</td>\n",
       "      <td>45.694237</td>\n",
       "      <td>0.139550</td>\n",
       "      <td>12.276641</td>\n",
       "      <td>62672.908339</td>\n",
       "      <td>241.659481</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009</td>\n",
       "      <td>1143</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>75.031250</td>\n",
       "      <td>27.093750</td>\n",
       "      <td>55.906250</td>\n",
       "      <td>6.343750</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483462</td>\n",
       "      <td>5.707081</td>\n",
       "      <td>24.857249</td>\n",
       "      <td>37.922101</td>\n",
       "      <td>0.101832</td>\n",
       "      <td>17.022045</td>\n",
       "      <td>56403.384404</td>\n",
       "      <td>586.620957</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>2010</td>\n",
       "      <td>1352</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>23.323529</td>\n",
       "      <td>53.323529</td>\n",
       "      <td>5.647059</td>\n",
       "      <td>15.470588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435349</td>\n",
       "      <td>5.966921</td>\n",
       "      <td>31.325718</td>\n",
       "      <td>35.567874</td>\n",
       "      <td>0.129745</td>\n",
       "      <td>14.418477</td>\n",
       "      <td>66317.665445</td>\n",
       "      <td>600.246113</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>2011</td>\n",
       "      <td>1242</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>82.382353</td>\n",
       "      <td>29.588235</td>\n",
       "      <td>57.617647</td>\n",
       "      <td>7.264706</td>\n",
       "      <td>18.764706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540804</td>\n",
       "      <td>7.140057</td>\n",
       "      <td>34.335500</td>\n",
       "      <td>38.461804</td>\n",
       "      <td>0.152372</td>\n",
       "      <td>10.118593</td>\n",
       "      <td>49478.637137</td>\n",
       "      <td>459.165885</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>2011</td>\n",
       "      <td>1228</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>71.281250</td>\n",
       "      <td>26.343750</td>\n",
       "      <td>56.343750</td>\n",
       "      <td>6.843750</td>\n",
       "      <td>17.687500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637593</td>\n",
       "      <td>7.101537</td>\n",
       "      <td>35.161898</td>\n",
       "      <td>40.164503</td>\n",
       "      <td>0.164143</td>\n",
       "      <td>15.802057</td>\n",
       "      <td>79118.599885</td>\n",
       "      <td>797.290386</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Season  lower_TeamID  lower_Wins  lower_Losses  \\\n",
       "1057    2019          1242          25             9   \n",
       "389     2009          1143          22            10   \n",
       "462     2010          1352          23            11   \n",
       "575     2011          1242          32             2   \n",
       "559     2011          1228          19            13   \n",
       "\n",
       "      lower_Winning Percentage  lower_Score_mean  lower_FGM_mean  \\\n",
       "1057                  0.735294         75.382353       27.294118   \n",
       "389                   0.687500         75.031250       27.093750   \n",
       "462                   0.676471         68.500000       23.323529   \n",
       "575                   0.941176         82.382353       29.588235   \n",
       "559                   0.593750         71.281250       26.343750   \n",
       "\n",
       "      lower_FGA_mean  lower_FGM3_mean  lower_FGA3_mean  ...  \\\n",
       "1057       59.058824         7.235294        20.647059  ...   \n",
       "389        55.906250         6.343750        14.625000  ...   \n",
       "462        53.323529         5.647059        15.470588  ...   \n",
       "575        57.617647         7.264706        18.764706  ...   \n",
       "559        56.343750         6.843750        17.687500  ...   \n",
       "\n",
       "      higher_AssistToTurnoverRatio_std  higher_Possessions_std  \\\n",
       "1057                          0.881175                7.485740   \n",
       "389                           0.483462                5.707081   \n",
       "462                           0.435349                5.966921   \n",
       "575                           0.540804                7.140057   \n",
       "559                           0.637593                7.101537   \n",
       "\n",
       "      higher_OffEff_std  higher_DefEff_std  higher_TO%_std  \\\n",
       "1057          25.135399          45.694237        0.139550   \n",
       "389           24.857249          37.922101        0.101832   \n",
       "462           31.325718          35.567874        0.129745   \n",
       "575           34.335500          38.461804        0.152372   \n",
       "559           35.161898          40.164503        0.164143   \n",
       "\n",
       "      higher_PointDiff_std  higher_OffensiveRating_std  \\\n",
       "1057             12.276641                62672.908339   \n",
       "389              17.022045                56403.384404   \n",
       "462              14.418477                66317.665445   \n",
       "575              10.118593                49478.637137   \n",
       "559              15.802057                79118.599885   \n",
       "\n",
       "      higher_DefensiveRating_std  Bracket  LowerWin?  \n",
       "1057                  241.659481        M          1  \n",
       "389                   586.620957        M          0  \n",
       "462                   600.246113        M          0  \n",
       "575                   459.165885        M          0  \n",
       "559                   797.290386        M          0  \n",
       "\n",
       "[5 rows x 191 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the training data\n",
    "training_data = pd.read_csv('/Users/jinalshah/Jinal/Projects/march-madness-mania/preprocessed-data/modeling-data/training.csv',index_col=0)\n",
    "\n",
    "# Making sure the data loaded correctly\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to make sure there aren't any missing values\n",
    "missing_vals = dict(training_data.isna().sum())\n",
    "\n",
    "# Iterating through the dictionary\n",
    "for col in missing_vals.keys():\n",
    "    if missing_vals[col] > 0:\n",
    "        print(f'Column {col} has {missing_vals[col]} missing values')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We need to preprocess the data a little bit so let's do that!\n",
    "\n",
    "Preprocessing that needs to be done:\n",
    "- Dropping the identifiers (lower_TeamID,higher_TeamID)\n",
    "- Converting Season into a number for how many seasons back the data is from (-1 = last season, -2 = 2 seasons ago, etc)\n",
    "- Converting Bracket into dummy variables\n",
    "- Scaling all numerical values by z-score to gain a normal distribution and all numbers on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the training set \n",
    "training_data_preprocessed = training_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unnecessary features\n",
    "training_data_preprocessed.drop(['lower_TeamID','higher_TeamID'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Season into numbers\n",
    "training_data_preprocessed['Season_converted'] = training_data_preprocessed['Season'] - 2023.0\n",
    "training_data_preprocessed.drop(['Season'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features matrix and target\n",
    "features = training_data_preprocessed.drop(['LowerWin?'],axis=1)\n",
    "target = training_data_preprocessed['LowerWin?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into numerical and categorical\n",
    "categorical = ['Bracket']\n",
    "numerical = list(features.columns)\n",
    "numerical.remove('Bracket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline to perform all the appropriate transformations\n",
    "preprocessing_pipeline = ColumnTransformer(transformers=[\n",
    "    ('scaler',StandardScaler(with_mean=True,with_std=True),numerical),\n",
    "    ('encoder',OneHotEncoder(),categorical)\n",
    "],remainder='passthrough',n_jobs=-1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] ....... (2 of 2) Processing encoder, total=   0.0s\n",
      "[ColumnTransformer] ........ (1 of 2) Processing scaler, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.23655977,  0.5006731 , -0.28877454, ...,  1.03353159,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.49451463,  0.78784489, -0.72021046, ..., -0.86041151,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.25082316,  1.07501668, -0.8197726 , ..., -0.6710172 ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.72394271, -0.93518583,  0.93594679, ..., -0.6710172 ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.96763417, -0.93518583,  0.97234585, ...,  1.03353159,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.96763417, -0.93518583,  0.97234585, ..., -0.48162289,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming the feature matrix via the pipeline\n",
    "features_preprocessed = preprocessing_pipeline.fit_transform(features)\n",
    "\n",
    "# Making sure fitting happened properly\n",
    "features_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is preprocessed!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Now that we have preprocessed the data for our models, it is time to build our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that holds various metrics\n",
    "metrics = {'Model':[],'Brier Score Training':[],'Mean Brier Score CV':[], 'Accuracy Training':[],'Mean Accuracy CV':[]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier\n",
    "\n",
    "This is our baseline. This classifier will simply randomly choose a class based on a uniform distribution of the class (0 or 1).\n",
    "We want our models to ideally be much better than this classifier, hence this is what we compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyClassifier(random_state=42, strategy=&#x27;uniform&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyClassifier</label><div class=\"sk-toggleable__content\"><pre>DummyClassifier(random_state=42, strategy=&#x27;uniform&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyClassifier(random_state=42, strategy='uniform')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the dummy classifier\n",
    "dummy_classifier = DummyClassifier(strategy='uniform',random_state=42)\n",
    "dummy_classifier.fit(features_preprocessed,target) # Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = dummy_classifier.predict(features_preprocessed)\n",
    "train_probs = dummy_classifier.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Dummy Classifier')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(dummy_classifier,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(dummy_classifier,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/dummy_classifier.sav','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression\n",
    "\n",
    "I am going to try a logisitic regression model. I don't expect this model to do too well since the I don't expect linear classification to be the answer to this problem. However, I do want to see how a non-regularized logistic regression model does. Expectation is that this model performs better than the dummy classifier by a significant margin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, n_jobs=-1, penalty=None, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, n_jobs=-1, penalty=None, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, n_jobs=-1, penalty=None, random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "logistic_reg = LogisticRegression(penalty=None,C=1.0,random_state=42,max_iter=1000,n_jobs=-1)\n",
    "logistic_reg.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = logistic_reg.predict(features_preprocessed)\n",
    "train_probs = logistic_reg.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Logistic Regression - No Regularization')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(logistic_reg,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(logistic_reg,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/logistic_reg_unregularized.sav','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Logistic Regression (Un-Regularized) did better than the dummy classifier. However, it did overfit a lot (which was expected) causing the CV Brier to not improve as much. Decision Trees are a much better model! I do expect overfitting; however, it will still lower the Mean Brier Score CV by a lot, I hope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini',random_state=42)\n",
    "decision_tree.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = decision_tree.predict(features_preprocessed)\n",
    "train_probs = decision_tree.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Decision Tree - UnRegularized')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(decision_tree,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(decision_tree,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/decision_tree_unregularized.sav','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Very interesting! Decision Trees overfit so bad that it performed worse or just marginally better than the dummy classifier. It did excellent on the training set but did so bad on the validation set. \n",
    "\n",
    "Let's see how Random Forests do; however, I expect them to do just as bad since its just a bagging of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_samples=0.7, n_estimators=1000, n_jobs=-1,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_samples=0.7, n_estimators=1000, n_jobs=-1,\n",
       "                       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_samples=0.7, n_estimators=1000, n_jobs=-1,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "random_forest = RandomForestClassifier(n_estimators=1000,criterion='gini',bootstrap=True,n_jobs=-1,random_state=42,max_samples=0.7)\n",
    "random_forest.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = random_forest.predict(features_preprocessed)\n",
    "train_probs = random_forest.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Random Forest - Basic')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(random_forest,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(random_forest,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/random_forest_basic.sav','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "Interesting, Random Forest performed a lot better. Granted, I did add some regularizars such as boostrapping and portion of training set to look at for each tree. Here, I am going to try AdaBoost. I am going to use AdaBoost for Logistic Regression and Decision Trees because I am curious how it will look."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(estimator=LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                penalty=None, random_state=42),\n",
       "                   learning_rate=1.5, n_estimators=500, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(estimator=LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                penalty=None, random_state=42),\n",
       "                   learning_rate=1.5, n_estimators=500, random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, n_jobs=-1, penalty=None, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, n_jobs=-1, penalty=None, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(estimator=LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                penalty=None, random_state=42),\n",
       "                   learning_rate=1.5, n_estimators=500, random_state=42)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "logistic_reg_ada = LogisticRegression(penalty=None,C=1.0,random_state=42,max_iter=1000,n_jobs=-1)\n",
    "adaboost_logistic = AdaBoostClassifier(estimator=logistic_reg_ada,n_estimators=500,learning_rate=1.5,random_state=42)\n",
    "adaboost_logistic.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = adaboost_logistic.predict(features_preprocessed)\n",
    "train_probs = adaboost_logistic.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Adaboost - Logistic Regression')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(adaboost_logistic,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(adaboost_logistic,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/adaboost_logistic.sav','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost - Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
       "                   learning_rate=1.5, n_estimators=500, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
       "                   learning_rate=1.5, n_estimators=500, random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
       "                   learning_rate=1.5, n_estimators=500, random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "decision_tree_ada = DecisionTreeClassifier(criterion='gini',random_state=42)\n",
    "adaboost_decision_tree = AdaBoostClassifier(estimator=decision_tree_ada,n_estimators=500,learning_rate=1.5,random_state=42)\n",
    "adaboost_decision_tree.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = adaboost_decision_tree.predict(features_preprocessed)\n",
    "train_probs = adaboost_decision_tree.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Adaboost - Decision Tree')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(adaboost_decision_tree,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(adaboost_decision_tree,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/adaboost_dtree.sav','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - Elastic Net\n",
    "\n",
    "I am going to try Logistic Regression with some regularization. I chose ElasticNet since I want some features to be 0'd via L-1 and weights to be small via L-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(l1_ratio=0.7, max_iter=5000, n_jobs=-1, penalty=&#x27;elasticnet&#x27;,\n",
       "                   random_state=42, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.7, max_iter=5000, n_jobs=-1, penalty=&#x27;elasticnet&#x27;,\n",
       "                   random_state=42, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(l1_ratio=0.7, max_iter=5000, n_jobs=-1, penalty='elasticnet',\n",
       "                   random_state=42, solver='saga')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "elastic_net = LogisticRegression(penalty='elasticnet',solver='saga',random_state=42,max_iter=5000,n_jobs=-1,l1_ratio=0.7)\n",
    "elastic_net.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = elastic_net.predict(features_preprocessed)\n",
    "train_probs = elastic_net.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Elastic Net - C = 1 and p = 0.7')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(elastic_net,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(elastic_net,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/elastic_net.sav','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Interesting, Logistic Regression with some regularization doesn't do much better than vanilla logistic regression. Furthermore, Random Forest still beats it. Another model to try is gradient boosting. Let's leverage CatBoost for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing some set-up required for catboost\n",
    "# Putting the data into a data pool for catboost\n",
    "training_pool = Pool(data=features_preprocessed,\n",
    "                     label=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6840838\ttotal: 43.5ms\tremaining: 3m 37s\n",
      "100:\tlearn: 0.4083271\ttotal: 7.56s\tremaining: 6m 6s\n",
      "200:\tlearn: 0.2717221\ttotal: 11.6s\tremaining: 4m 37s\n",
      "300:\tlearn: 0.1662506\ttotal: 14.6s\tremaining: 3m 48s\n",
      "400:\tlearn: 0.1068114\ttotal: 16.8s\tremaining: 3m 12s\n",
      "500:\tlearn: 0.0717925\ttotal: 19.7s\tremaining: 2m 57s\n",
      "600:\tlearn: 0.0504900\ttotal: 21.9s\tremaining: 2m 40s\n",
      "700:\tlearn: 0.0368315\ttotal: 24.9s\tremaining: 2m 32s\n",
      "800:\tlearn: 0.0275209\ttotal: 27.1s\tremaining: 2m 22s\n",
      "900:\tlearn: 0.0211331\ttotal: 30.3s\tremaining: 2m 17s\n",
      "1000:\tlearn: 0.0171264\ttotal: 32.3s\tremaining: 2m 8s\n",
      "1100:\tlearn: 0.0139857\ttotal: 35.5s\tremaining: 2m 5s\n",
      "1200:\tlearn: 0.0115468\ttotal: 37.3s\tremaining: 1m 57s\n",
      "1300:\tlearn: 0.0097986\ttotal: 40.6s\tremaining: 1m 55s\n",
      "1400:\tlearn: 0.0085618\ttotal: 42.4s\tremaining: 1m 49s\n",
      "1500:\tlearn: 0.0075087\ttotal: 46s\tremaining: 1m 47s\n",
      "1600:\tlearn: 0.0067199\ttotal: 48.7s\tremaining: 1m 43s\n",
      "1700:\tlearn: 0.0060547\ttotal: 58.4s\tremaining: 1m 53s\n",
      "1800:\tlearn: 0.0055062\ttotal: 1m 5s\tremaining: 1m 56s\n",
      "1900:\tlearn: 0.0052286\ttotal: 1m 10s\tremaining: 1m 54s\n",
      "2000:\tlearn: 0.0049664\ttotal: 1m 13s\tremaining: 1m 49s\n",
      "2100:\tlearn: 0.0047419\ttotal: 1m 16s\tremaining: 1m 45s\n",
      "2200:\tlearn: 0.0045898\ttotal: 1m 21s\tremaining: 1m 43s\n",
      "2300:\tlearn: 0.0044481\ttotal: 1m 25s\tremaining: 1m 40s\n",
      "2400:\tlearn: 0.0043543\ttotal: 1m 29s\tremaining: 1m 37s\n",
      "2500:\tlearn: 0.0042998\ttotal: 1m 31s\tremaining: 1m 31s\n",
      "2600:\tlearn: 0.0042434\ttotal: 1m 35s\tremaining: 1m 27s\n",
      "2700:\tlearn: 0.0041790\ttotal: 1m 37s\tremaining: 1m 22s\n",
      "2800:\tlearn: 0.0041346\ttotal: 1m 40s\tremaining: 1m 18s\n",
      "2900:\tlearn: 0.0040850\ttotal: 1m 42s\tremaining: 1m 14s\n",
      "3000:\tlearn: 0.0040655\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "3100:\tlearn: 0.0040347\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "3200:\tlearn: 0.0039622\ttotal: 1m 51s\tremaining: 1m 2s\n",
      "3300:\tlearn: 0.0039150\ttotal: 1m 55s\tremaining: 59.7s\n",
      "3400:\tlearn: 0.0038781\ttotal: 1m 59s\tremaining: 56.4s\n",
      "3500:\tlearn: 0.0038272\ttotal: 2m 2s\tremaining: 52.6s\n",
      "3600:\tlearn: 0.0037987\ttotal: 2m 6s\tremaining: 49.2s\n",
      "3700:\tlearn: 0.0037673\ttotal: 2m 10s\tremaining: 45.9s\n",
      "3800:\tlearn: 0.0037396\ttotal: 2m 14s\tremaining: 42.5s\n",
      "3900:\tlearn: 0.0037081\ttotal: 2m 17s\tremaining: 38.8s\n",
      "4000:\tlearn: 0.0036910\ttotal: 2m 20s\tremaining: 35.2s\n",
      "4100:\tlearn: 0.0036614\ttotal: 2m 24s\tremaining: 31.6s\n",
      "4200:\tlearn: 0.0036481\ttotal: 2m 26s\tremaining: 27.8s\n",
      "4300:\tlearn: 0.0036245\ttotal: 2m 29s\tremaining: 24.3s\n",
      "4400:\tlearn: 0.0036162\ttotal: 2m 31s\tremaining: 20.6s\n",
      "4500:\tlearn: 0.0036015\ttotal: 2m 34s\tremaining: 17.2s\n",
      "4600:\tlearn: 0.0035609\ttotal: 2m 37s\tremaining: 13.7s\n",
      "4700:\tlearn: 0.0035561\ttotal: 2m 40s\tremaining: 10.2s\n",
      "4800:\tlearn: 0.0035364\ttotal: 2m 44s\tremaining: 6.8s\n",
      "4900:\tlearn: 0.0035084\ttotal: 2m 45s\tremaining: 3.35s\n",
      "4999:\tlearn: 0.0034941\ttotal: 2m 49s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "catboost_clf = CatBoostClassifier(iterations=5000,learning_rate=0.05,loss_function='Logloss',random_seed=42,verbose=100,\n",
    "                                  early_stopping_rounds=5)\n",
    "\n",
    "# Fitting the model\n",
    "catboost_clf.fit(training_pool)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = catboost_clf.predict(features_preprocessed,prediction_type='Class',ntree_start=0,ntree_end=catboost_clf.tree_count_ - 1)\n",
    "train_probs = catboost_clf.predict_proba(features_preprocessed,ntree_start=0,ntree_end=catboost_clf.tree_count_ - 1)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Catboost - Gradient Boosting')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold [0/5]\n",
      "0:\tlearn: 0.6835129\ttest: 0.6873463\tbest: 0.6873463 (0)\ttotal: 996ms\tremaining: 1h 22m 57s\n",
      "200:\tlearn: 0.2275412\ttest: 0.6385977\tbest: 0.6227853 (84)\ttotal: 7.6s\tremaining: 3m 1s\n",
      "400:\tlearn: 0.0764722\ttest: 0.6916963\tbest: 0.6227853 (84)\ttotal: 12.5s\tremaining: 2m 23s\n",
      "600:\tlearn: 0.0336975\ttest: 0.7383267\tbest: 0.6227853 (84)\ttotal: 17.2s\tremaining: 2m 6s\n",
      "800:\tlearn: 0.0182806\ttest: 0.7824567\tbest: 0.6227853 (84)\ttotal: 22.2s\tremaining: 1m 56s\n",
      "1000:\tlearn: 0.0114115\ttest: 0.8082527\tbest: 0.6227853 (84)\ttotal: 27.1s\tremaining: 1m 48s\n",
      "1200:\tlearn: 0.0079277\ttest: 0.8371343\tbest: 0.6227853 (84)\ttotal: 32.1s\tremaining: 1m 41s\n",
      "1400:\tlearn: 0.0058967\ttest: 0.8566901\tbest: 0.6227853 (84)\ttotal: 37s\tremaining: 1m 35s\n",
      "1600:\tlearn: 0.0049243\ttest: 0.8695806\tbest: 0.6227853 (84)\ttotal: 42s\tremaining: 1m 29s\n",
      "1800:\tlearn: 0.0044053\ttest: 0.8769341\tbest: 0.6227853 (84)\ttotal: 47s\tremaining: 1m 23s\n",
      "2000:\tlearn: 0.0041397\ttest: 0.8826590\tbest: 0.6227853 (84)\ttotal: 52s\tremaining: 1m 17s\n",
      "2200:\tlearn: 0.0039575\ttest: 0.8862701\tbest: 0.6227853 (84)\ttotal: 1m\tremaining: 1m 16s\n",
      "2400:\tlearn: 0.0037969\ttest: 0.8887636\tbest: 0.6227853 (84)\ttotal: 1m 5s\tremaining: 1m 10s\n",
      "2600:\tlearn: 0.0036810\ttest: 0.8912360\tbest: 0.6227853 (84)\ttotal: 1m 10s\tremaining: 1m 4s\n",
      "2800:\tlearn: 0.0035795\ttest: 0.8932855\tbest: 0.6227853 (84)\ttotal: 1m 15s\tremaining: 59s\n",
      "3000:\tlearn: 0.0034971\ttest: 0.8946963\tbest: 0.6227853 (84)\ttotal: 1m 20s\tremaining: 53.4s\n",
      "3200:\tlearn: 0.0034475\ttest: 0.8956982\tbest: 0.6227853 (84)\ttotal: 1m 25s\tremaining: 47.9s\n",
      "3400:\tlearn: 0.0034307\ttest: 0.8962058\tbest: 0.6227853 (84)\ttotal: 1m 30s\tremaining: 42.4s\n",
      "3600:\tlearn: 0.0033598\ttest: 0.8976068\tbest: 0.6227853 (84)\ttotal: 1m 35s\tremaining: 37s\n",
      "3800:\tlearn: 0.0033253\ttest: 0.8980871\tbest: 0.6227853 (84)\ttotal: 1m 40s\tremaining: 31.6s\n",
      "4000:\tlearn: 0.0032876\ttest: 0.8990824\tbest: 0.6227853 (84)\ttotal: 1m 45s\tremaining: 26.3s\n",
      "4200:\tlearn: 0.0032608\ttest: 0.8997681\tbest: 0.6227853 (84)\ttotal: 1m 50s\tremaining: 21s\n",
      "4400:\tlearn: 0.0032376\ttest: 0.9001717\tbest: 0.6227853 (84)\ttotal: 1m 55s\tremaining: 15.7s\n",
      "4600:\tlearn: 0.0031953\ttest: 0.9010273\tbest: 0.6227853 (84)\ttotal: 2m\tremaining: 10.4s\n",
      "4800:\tlearn: 0.0031513\ttest: 0.9015553\tbest: 0.6227853 (84)\ttotal: 2m 6s\tremaining: 5.24s\n",
      "4999:\tlearn: 0.0031111\ttest: 0.9019821\tbest: 0.6227853 (84)\ttotal: 2m 12s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.6227852583\n",
      "bestIteration = 84\n",
      "\n",
      "Training on fold [1/5]\n",
      "0:\tlearn: 0.6806179\ttest: 0.6843052\tbest: 0.6843052 (0)\ttotal: 30.4ms\tremaining: 2m 32s\n",
      "200:\tlearn: 0.2316616\ttest: 0.5712303\tbest: 0.5712303 (200)\ttotal: 6.5s\tremaining: 2m 35s\n",
      "400:\tlearn: 0.0800524\ttest: 0.5939349\tbest: 0.5708577 (202)\ttotal: 12s\tremaining: 2m 17s\n",
      "600:\tlearn: 0.0353563\ttest: 0.6239515\tbest: 0.5708577 (202)\ttotal: 18.5s\tremaining: 2m 15s\n",
      "800:\tlearn: 0.0200856\ttest: 0.6527717\tbest: 0.5708577 (202)\ttotal: 27.6s\tremaining: 2m 24s\n",
      "1000:\tlearn: 0.0130025\ttest: 0.6828118\tbest: 0.5708577 (202)\ttotal: 32s\tremaining: 2m 7s\n",
      "1200:\tlearn: 0.0090167\ttest: 0.7066767\tbest: 0.5708577 (202)\ttotal: 35.9s\tremaining: 1m 53s\n",
      "1400:\tlearn: 0.0066486\ttest: 0.7189058\tbest: 0.5708577 (202)\ttotal: 39.4s\tremaining: 1m 41s\n",
      "1600:\tlearn: 0.0054369\ttest: 0.7333197\tbest: 0.5708577 (202)\ttotal: 43.7s\tremaining: 1m 32s\n",
      "1800:\tlearn: 0.0047074\ttest: 0.7421999\tbest: 0.5708577 (202)\ttotal: 47.9s\tremaining: 1m 24s\n",
      "2000:\tlearn: 0.0042691\ttest: 0.7478433\tbest: 0.5708577 (202)\ttotal: 52.1s\tremaining: 1m 18s\n",
      "2200:\tlearn: 0.0040289\ttest: 0.7534530\tbest: 0.5708577 (202)\ttotal: 56.4s\tremaining: 1m 11s\n",
      "2400:\tlearn: 0.0038366\ttest: 0.7573248\tbest: 0.5708577 (202)\ttotal: 59.6s\tremaining: 1m 4s\n",
      "2600:\tlearn: 0.0037102\ttest: 0.7600729\tbest: 0.5708577 (202)\ttotal: 1m 3s\tremaining: 58.9s\n",
      "2800:\tlearn: 0.0036429\ttest: 0.7615742\tbest: 0.5708577 (202)\ttotal: 1m 7s\tremaining: 53.4s\n",
      "3000:\tlearn: 0.0035655\ttest: 0.7633022\tbest: 0.5708577 (202)\ttotal: 1m 12s\tremaining: 48.2s\n",
      "3200:\tlearn: 0.0034835\ttest: 0.7640035\tbest: 0.5708577 (202)\ttotal: 1m 16s\tremaining: 43.1s\n",
      "3400:\tlearn: 0.0034363\ttest: 0.7658155\tbest: 0.5708577 (202)\ttotal: 1m 21s\tremaining: 38.1s\n",
      "3600:\tlearn: 0.0033911\ttest: 0.7668472\tbest: 0.5708577 (202)\ttotal: 1m 24s\tremaining: 32.7s\n",
      "3800:\tlearn: 0.0033514\ttest: 0.7678263\tbest: 0.5708577 (202)\ttotal: 1m 28s\tremaining: 27.9s\n",
      "4000:\tlearn: 0.0033137\ttest: 0.7680946\tbest: 0.5708577 (202)\ttotal: 1m 34s\tremaining: 23.5s\n",
      "4200:\tlearn: 0.0032819\ttest: 0.7691133\tbest: 0.5708577 (202)\ttotal: 1m 42s\tremaining: 19.5s\n",
      "4400:\tlearn: 0.0032487\ttest: 0.7697873\tbest: 0.5708577 (202)\ttotal: 1m 48s\tremaining: 14.7s\n",
      "4600:\tlearn: 0.0032267\ttest: 0.7702924\tbest: 0.5708577 (202)\ttotal: 1m 53s\tremaining: 9.86s\n",
      "4800:\tlearn: 0.0032114\ttest: 0.7705104\tbest: 0.5708577 (202)\ttotal: 1m 59s\tremaining: 4.94s\n",
      "4999:\tlearn: 0.0031764\ttest: 0.7708119\tbest: 0.5708577 (202)\ttotal: 2m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5708576896\n",
      "bestIteration = 202\n",
      "\n",
      "Training on fold [2/5]\n",
      "0:\tlearn: 0.6854853\ttest: 0.6869158\tbest: 0.6869158 (0)\ttotal: 146ms\tremaining: 12m 11s\n",
      "200:\tlearn: 0.2322157\ttest: 0.6032451\tbest: 0.5903235 (98)\ttotal: 6.73s\tremaining: 2m 40s\n",
      "400:\tlearn: 0.0816615\ttest: 0.6383218\tbest: 0.5903235 (98)\ttotal: 12.5s\tremaining: 2m 23s\n",
      "600:\tlearn: 0.0356891\ttest: 0.6892381\tbest: 0.5903235 (98)\ttotal: 18s\tremaining: 2m 12s\n",
      "800:\tlearn: 0.0188917\ttest: 0.7255913\tbest: 0.5903235 (98)\ttotal: 23.4s\tremaining: 2m 2s\n",
      "1000:\tlearn: 0.0118189\ttest: 0.7604293\tbest: 0.5903235 (98)\ttotal: 30.3s\tremaining: 2m\n",
      "1200:\tlearn: 0.0082168\ttest: 0.7898693\tbest: 0.5903235 (98)\ttotal: 36.2s\tremaining: 1m 54s\n",
      "1400:\tlearn: 0.0060905\ttest: 0.8117198\tbest: 0.5903235 (98)\ttotal: 41.5s\tremaining: 1m 46s\n",
      "1600:\tlearn: 0.0049563\ttest: 0.8297377\tbest: 0.5903235 (98)\ttotal: 47s\tremaining: 1m 39s\n",
      "1800:\tlearn: 0.0043268\ttest: 0.8427498\tbest: 0.5903235 (98)\ttotal: 52.2s\tremaining: 1m 32s\n",
      "2000:\tlearn: 0.0040309\ttest: 0.8486988\tbest: 0.5903235 (98)\ttotal: 57.6s\tremaining: 1m 26s\n",
      "2200:\tlearn: 0.0038183\ttest: 0.8535700\tbest: 0.5903235 (98)\ttotal: 1m 2s\tremaining: 1m 19s\n",
      "2400:\tlearn: 0.0037241\ttest: 0.8570769\tbest: 0.5903235 (98)\ttotal: 1m 8s\tremaining: 1m 13s\n",
      "2600:\tlearn: 0.0036529\ttest: 0.8589570\tbest: 0.5903235 (98)\ttotal: 1m 13s\tremaining: 1m 7s\n",
      "2800:\tlearn: 0.0035973\ttest: 0.8608441\tbest: 0.5903235 (98)\ttotal: 1m 18s\tremaining: 1m 1s\n",
      "3000:\tlearn: 0.0035297\ttest: 0.8630101\tbest: 0.5903235 (98)\ttotal: 1m 25s\tremaining: 56.9s\n",
      "3200:\tlearn: 0.0034683\ttest: 0.8637585\tbest: 0.5903235 (98)\ttotal: 1m 30s\tremaining: 51s\n",
      "3400:\tlearn: 0.0034062\ttest: 0.8648336\tbest: 0.5903235 (98)\ttotal: 1m 36s\tremaining: 45.3s\n",
      "3600:\tlearn: 0.0033618\ttest: 0.8651532\tbest: 0.5903235 (98)\ttotal: 1m 41s\tremaining: 39.5s\n",
      "3800:\tlearn: 0.0033217\ttest: 0.8658979\tbest: 0.5903235 (98)\ttotal: 1m 46s\tremaining: 33.7s\n",
      "4000:\tlearn: 0.0032918\ttest: 0.8671659\tbest: 0.5903235 (98)\ttotal: 1m 52s\tremaining: 28s\n",
      "4200:\tlearn: 0.0032499\ttest: 0.8689253\tbest: 0.5903235 (98)\ttotal: 1m 57s\tremaining: 22.4s\n",
      "4400:\tlearn: 0.0032180\ttest: 0.8697664\tbest: 0.5903235 (98)\ttotal: 2m 2s\tremaining: 16.7s\n",
      "4600:\tlearn: 0.0031869\ttest: 0.8709770\tbest: 0.5903235 (98)\ttotal: 2m 8s\tremaining: 11.1s\n",
      "4800:\tlearn: 0.0031640\ttest: 0.8713224\tbest: 0.5903235 (98)\ttotal: 2m 15s\tremaining: 5.61s\n",
      "4999:\tlearn: 0.0031398\ttest: 0.8723141\tbest: 0.5903235 (98)\ttotal: 2m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5903234641\n",
      "bestIteration = 98\n",
      "\n",
      "Training on fold [3/5]\n",
      "0:\tlearn: 0.6852176\ttest: 0.6871597\tbest: 0.6871597 (0)\ttotal: 26.3ms\tremaining: 2m 11s\n",
      "200:\tlearn: 0.2411308\ttest: 0.5713669\tbest: 0.5642979 (158)\ttotal: 5.93s\tremaining: 2m 21s\n",
      "400:\tlearn: 0.0849403\ttest: 0.5905513\tbest: 0.5642979 (158)\ttotal: 11.5s\tremaining: 2m 11s\n",
      "600:\tlearn: 0.0381402\ttest: 0.6176015\tbest: 0.5642979 (158)\ttotal: 16.9s\tremaining: 2m 3s\n",
      "800:\tlearn: 0.0201840\ttest: 0.6528759\tbest: 0.5642979 (158)\ttotal: 22.7s\tremaining: 1m 58s\n",
      "1000:\tlearn: 0.0127579\ttest: 0.6810560\tbest: 0.5642979 (158)\ttotal: 29.4s\tremaining: 1m 57s\n",
      "1200:\tlearn: 0.0086733\ttest: 0.7034037\tbest: 0.5642979 (158)\ttotal: 34.8s\tremaining: 1m 50s\n",
      "1400:\tlearn: 0.0063832\ttest: 0.7278042\tbest: 0.5642979 (158)\ttotal: 40.3s\tremaining: 1m 43s\n",
      "1600:\tlearn: 0.0052959\ttest: 0.7418736\tbest: 0.5642979 (158)\ttotal: 45.8s\tremaining: 1m 37s\n",
      "1800:\tlearn: 0.0045973\ttest: 0.7508053\tbest: 0.5642979 (158)\ttotal: 51.2s\tremaining: 1m 30s\n",
      "2000:\tlearn: 0.0042374\ttest: 0.7544574\tbest: 0.5642979 (158)\ttotal: 59.9s\tremaining: 1m 29s\n",
      "2200:\tlearn: 0.0040343\ttest: 0.7551664\tbest: 0.5642979 (158)\ttotal: 1m 5s\tremaining: 1m 23s\n",
      "2400:\tlearn: 0.0038378\ttest: 0.7599742\tbest: 0.5642979 (158)\ttotal: 1m 10s\tremaining: 1m 16s\n",
      "2600:\tlearn: 0.0036876\ttest: 0.7641210\tbest: 0.5642979 (158)\ttotal: 1m 16s\tremaining: 1m 10s\n",
      "2800:\tlearn: 0.0035878\ttest: 0.7654705\tbest: 0.5642979 (158)\ttotal: 1m 21s\tremaining: 1m 3s\n",
      "3000:\tlearn: 0.0035207\ttest: 0.7662297\tbest: 0.5642979 (158)\ttotal: 1m 26s\tremaining: 57.9s\n",
      "3200:\tlearn: 0.0034377\ttest: 0.7684393\tbest: 0.5642979 (158)\ttotal: 1m 32s\tremaining: 51.9s\n",
      "3400:\tlearn: 0.0033703\ttest: 0.7700996\tbest: 0.5642979 (158)\ttotal: 1m 38s\tremaining: 46.5s\n",
      "3600:\tlearn: 0.0033213\ttest: 0.7712201\tbest: 0.5642979 (158)\ttotal: 1m 44s\tremaining: 40.6s\n",
      "3800:\tlearn: 0.0032991\ttest: 0.7715810\tbest: 0.5642979 (158)\ttotal: 1m 49s\tremaining: 34.6s\n",
      "4000:\tlearn: 0.0032789\ttest: 0.7720911\tbest: 0.5642979 (158)\ttotal: 1m 55s\tremaining: 28.8s\n",
      "4200:\tlearn: 0.0032439\ttest: 0.7724772\tbest: 0.5642979 (158)\ttotal: 2m\tremaining: 23s\n",
      "4400:\tlearn: 0.0032221\ttest: 0.7728510\tbest: 0.5642979 (158)\ttotal: 2m 6s\tremaining: 17.2s\n",
      "4600:\tlearn: 0.0031882\ttest: 0.7735795\tbest: 0.5642979 (158)\ttotal: 2m 12s\tremaining: 11.5s\n",
      "4800:\tlearn: 0.0031756\ttest: 0.7741082\tbest: 0.5642979 (158)\ttotal: 2m 19s\tremaining: 5.8s\n",
      "4999:\tlearn: 0.0031532\ttest: 0.7749035\tbest: 0.5642979 (158)\ttotal: 2m 30s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5642979124\n",
      "bestIteration = 158\n",
      "\n",
      "Training on fold [4/5]\n",
      "0:\tlearn: 0.6821024\ttest: 0.6852690\tbest: 0.6852690 (0)\ttotal: 34.7ms\tremaining: 2m 53s\n",
      "200:\tlearn: 0.2217605\ttest: 0.6781960\tbest: 0.6266544 (39)\ttotal: 8.95s\tremaining: 3m 33s\n",
      "400:\tlearn: 0.0738255\ttest: 0.7509208\tbest: 0.6266544 (39)\ttotal: 15.7s\tremaining: 3m\n",
      "600:\tlearn: 0.0328495\ttest: 0.8077838\tbest: 0.6266544 (39)\ttotal: 20.4s\tremaining: 2m 29s\n",
      "800:\tlearn: 0.0173785\ttest: 0.8623670\tbest: 0.6266544 (39)\ttotal: 25.2s\tremaining: 2m 11s\n",
      "1000:\tlearn: 0.0107810\ttest: 0.9106007\tbest: 0.6266544 (39)\ttotal: 29.9s\tremaining: 1m 59s\n",
      "1200:\tlearn: 0.0074794\ttest: 0.9507594\tbest: 0.6266544 (39)\ttotal: 34.7s\tremaining: 1m 49s\n",
      "1400:\tlearn: 0.0060616\ttest: 0.9710055\tbest: 0.6266544 (39)\ttotal: 39.2s\tremaining: 1m 40s\n",
      "1600:\tlearn: 0.0050958\ttest: 0.9882675\tbest: 0.6266544 (39)\ttotal: 43.9s\tremaining: 1m 33s\n",
      "1800:\tlearn: 0.0045880\ttest: 1.0024057\tbest: 0.6266544 (39)\ttotal: 48.6s\tremaining: 1m 26s\n",
      "2000:\tlearn: 0.0043152\ttest: 1.0072610\tbest: 0.6266544 (39)\ttotal: 53.2s\tremaining: 1m 19s\n",
      "2200:\tlearn: 0.0041170\ttest: 1.0124467\tbest: 0.6266544 (39)\ttotal: 57.5s\tremaining: 1m 13s\n",
      "2400:\tlearn: 0.0039692\ttest: 1.0156264\tbest: 0.6266544 (39)\ttotal: 1m 1s\tremaining: 1m 6s\n",
      "2600:\tlearn: 0.0038249\ttest: 1.0211460\tbest: 0.6266544 (39)\ttotal: 1m 6s\tremaining: 1m 1s\n",
      "2800:\tlearn: 0.0037371\ttest: 1.0244124\tbest: 0.6266544 (39)\ttotal: 1m 11s\tremaining: 55.8s\n",
      "3000:\tlearn: 0.0036535\ttest: 1.0270280\tbest: 0.6266544 (39)\ttotal: 1m 15s\tremaining: 50.5s\n",
      "3200:\tlearn: 0.0035683\ttest: 1.0298582\tbest: 0.6266544 (39)\ttotal: 1m 20s\tremaining: 45.4s\n",
      "3400:\tlearn: 0.0035110\ttest: 1.0310731\tbest: 0.6266544 (39)\ttotal: 1m 25s\tremaining: 40.2s\n",
      "3600:\tlearn: 0.0034220\ttest: 1.0345419\tbest: 0.6266544 (39)\ttotal: 1m 30s\tremaining: 35s\n",
      "3800:\tlearn: 0.0033880\ttest: 1.0352977\tbest: 0.6266544 (39)\ttotal: 1m 35s\tremaining: 30.2s\n",
      "4000:\tlearn: 0.0033403\ttest: 1.0365630\tbest: 0.6266544 (39)\ttotal: 1m 40s\tremaining: 25.1s\n",
      "4200:\tlearn: 0.0032885\ttest: 1.0388713\tbest: 0.6266544 (39)\ttotal: 1m 46s\tremaining: 20.2s\n",
      "4400:\tlearn: 0.0032553\ttest: 1.0400178\tbest: 0.6266544 (39)\ttotal: 1m 52s\tremaining: 15.4s\n",
      "4600:\tlearn: 0.0032240\ttest: 1.0399803\tbest: 0.6266544 (39)\ttotal: 1m 57s\tremaining: 10.2s\n",
      "4800:\tlearn: 0.0031923\ttest: 1.0412313\tbest: 0.6266544 (39)\ttotal: 2m 1s\tremaining: 5.02s\n",
      "4999:\tlearn: 0.0031572\ttest: 1.0429384\tbest: 0.6266544 (39)\ttotal: 2m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.626654384\n",
      "bestIteration = 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting up for cross validation\n",
    "params = {'iterations':5000,'learning_rate':0.05,'loss_function':'Logloss',\n",
    "            'random_seed':42,'verbose':200,\n",
    "            'custom_metric':['BrierScore','Accuracy']}\n",
    "scores = cv(training_pool,params,fold_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append(scores.loc[4999,'test-BrierScore-mean'])\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append(scores.loc[4999,'test-Accuracy-mean'])\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "catboost_clf.save_model('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/catboost_vanilla.cbm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "CatBoost did ok, overfit heavy (which was expected) but this overfitting caused the Brier Score to be worse than a random model. I am going to try one more model, a neural network, before I start testing some ensembles and fine-tune some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 300)               57000     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 500)               150500    \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 300)               150300    \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 150)               45150     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 75)                11325     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 76        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 414,351\n",
      "Trainable params: 414,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the neural network\n",
    "neural_net = keras.Sequential([\n",
    "    keras.Input(shape=(features_preprocessed.shape[1])), # input layer\n",
    "    layers.Dense(300,activation='relu'),\n",
    "    layers.Dense(500,activation='relu'),\n",
    "    layers.Dense(300,activation='relu'),\n",
    "    layers.Dense(150,activation='relu'),\n",
    "    layers.Dense(75,activation='relu'),\n",
    "    layers.Dense(1,activation='softmax'),\n",
    "])\n",
    "\n",
    "neural_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leveragin SciKeras so we can use sklearn functions on the model\n",
    "neural_net_wrapped = KerasClassifier(model=neural_net,optimizer='adam',loss='binary_crossentropy',random_state=42,batch_size=32,\n",
    "                                     optimizer__learning_rate=0.03,epochs=500,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "51/51 [==============================] - 7s 11ms/step - loss: 6.6406\n",
      "Epoch 2/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6933\n",
      "Epoch 3/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6620\n",
      "Epoch 4/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6835\n",
      "Epoch 5/500\n",
      "51/51 [==============================] - 0s 10ms/step - loss: 0.6951\n",
      "Epoch 6/500\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 7/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 8/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 9/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 10/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 11/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 12/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 13/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 14/500\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.6946\n",
      "Epoch 15/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 16/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6950\n",
      "Epoch 17/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 18/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 19/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 20/500\n",
      "51/51 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      "Epoch 21/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 22/500\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.6934\n",
      "Epoch 23/500\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.6949\n",
      "Epoch 24/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 25/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 26/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 27/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 28/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6950\n",
      "Epoch 29/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 30/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 31/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 32/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 33/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 34/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 35/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 36/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 37/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 38/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 39/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6939\n",
      "Epoch 40/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 41/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 42/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6950\n",
      "Epoch 43/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 44/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6942\n",
      "Epoch 45/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 46/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 47/500\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.6942\n",
      "Epoch 48/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 49/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 50/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 51/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 52/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 53/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 54/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 55/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 56/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 57/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 58/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 59/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 60/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 61/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 62/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 63/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 64/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 65/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 66/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 67/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 68/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 69/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6942\n",
      "Epoch 70/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6962\n",
      "Epoch 71/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 72/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 73/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6950\n",
      "Epoch 74/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 75/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 76/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 77/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6932\n",
      "Epoch 78/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 79/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 80/500\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.6942\n",
      "Epoch 81/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 82/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 83/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 84/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 85/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 86/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 87/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 88/500\n",
      "51/51 [==============================] - 0s 10ms/step - loss: 0.6945\n",
      "Epoch 89/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 90/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6949\n",
      "Epoch 91/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 92/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 93/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 94/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 95/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 96/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 97/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 98/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 99/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 100/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 101/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 102/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 103/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6946\n",
      "Epoch 104/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 105/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6933\n",
      "Epoch 106/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 107/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 108/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 109/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 110/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 111/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 112/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 113/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 114/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 115/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 116/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 117/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 118/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 119/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 120/500\n",
      "51/51 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      "Epoch 121/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 122/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 123/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 124/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 125/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 126/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 127/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6949\n",
      "Epoch 128/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 129/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 130/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 131/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 132/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 133/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 134/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6948\n",
      "Epoch 135/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 136/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 137/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 138/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 139/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 140/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 141/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 142/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 143/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 144/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 145/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 146/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 147/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 148/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 149/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6934\n",
      "Epoch 150/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 151/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 152/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 153/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 154/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 155/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 156/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 157/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 158/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 159/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 160/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 161/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 162/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 163/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6962\n",
      "Epoch 164/500\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.6934\n",
      "Epoch 165/500\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.6940\n",
      "Epoch 166/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 167/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 168/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 169/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6933\n",
      "Epoch 170/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 171/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6954\n",
      "Epoch 172/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 173/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 174/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 175/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 176/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 177/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 178/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 179/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 180/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6947\n",
      "Epoch 181/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 182/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 183/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 184/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6946\n",
      "Epoch 185/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 186/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 187/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 188/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 189/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 190/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 191/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 192/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 193/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 194/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 195/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 196/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 197/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6933\n",
      "Epoch 198/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 199/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6942\n",
      "Epoch 200/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 201/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6947\n",
      "Epoch 202/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 203/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 204/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 205/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 206/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 207/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6944\n",
      "Epoch 208/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 209/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 210/500\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.6938\n",
      "Epoch 211/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 212/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 213/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6945\n",
      "Epoch 214/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 215/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 216/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 217/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 218/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 219/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 220/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 221/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 222/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 223/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 224/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 225/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 226/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 227/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 228/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 229/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 230/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 231/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 232/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6935\n",
      "Epoch 233/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 234/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 235/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 236/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 237/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 238/500\n",
      "51/51 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      "Epoch 239/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6944\n",
      "Epoch 240/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 241/500\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.6937\n",
      "Epoch 242/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 243/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 244/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 245/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 246/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 247/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 248/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 249/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 250/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 251/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 252/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 253/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 254/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6937\n",
      "Epoch 255/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 256/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 257/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6943\n",
      "Epoch 258/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 259/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6947\n",
      "Epoch 260/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6944\n",
      "Epoch 261/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 262/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 263/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 264/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 265/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 266/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6943\n",
      "Epoch 267/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6933\n",
      "Epoch 268/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6949\n",
      "Epoch 269/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 270/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 271/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 272/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 273/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 274/500\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 275/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 276/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 277/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 278/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 279/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 280/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 281/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6938\n",
      "Epoch 282/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 283/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 284/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 285/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 286/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6947\n",
      "Epoch 287/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 288/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 289/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 290/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 291/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 292/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 293/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 294/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6949\n",
      "Epoch 295/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 296/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 297/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 298/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 299/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 300/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 301/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 302/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 303/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 304/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 305/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 306/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 307/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 308/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6948\n",
      "Epoch 309/500\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 310/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 311/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 312/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 313/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 314/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6945\n",
      "Epoch 315/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6952\n",
      "Epoch 316/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 317/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 318/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 319/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 320/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 321/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 322/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 323/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 324/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 325/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 326/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 327/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 328/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 329/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6934\n",
      "Epoch 330/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 331/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6948\n",
      "Epoch 332/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 333/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 334/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 335/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 336/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 337/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6943\n",
      "Epoch 338/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 339/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 340/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 341/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6945\n",
      "Epoch 342/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 343/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6943\n",
      "Epoch 344/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 345/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 346/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 347/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 348/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6952\n",
      "Epoch 349/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 350/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6932\n",
      "Epoch 351/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 352/500\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.6941\n",
      "Epoch 353/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 354/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 355/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 356/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6944\n",
      "Epoch 357/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 358/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 359/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 360/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 361/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 362/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 363/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 364/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 365/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 366/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 367/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 368/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6947\n",
      "Epoch 369/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 370/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 371/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6944\n",
      "Epoch 372/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 373/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 374/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 375/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 376/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 377/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6960\n",
      "Epoch 378/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 379/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 380/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 381/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 382/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6953\n",
      "Epoch 383/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6928\n",
      "Epoch 384/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 385/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 386/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6935\n",
      "Epoch 387/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6933\n",
      "Epoch 388/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6946\n",
      "Epoch 389/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6946\n",
      "Epoch 390/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 391/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 392/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 393/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 394/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6944\n",
      "Epoch 395/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 396/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 397/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 398/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 399/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 400/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6948\n",
      "Epoch 401/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 402/500\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.6948\n",
      "Epoch 403/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 404/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6932\n",
      "Epoch 405/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 406/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 407/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6945\n",
      "Epoch 408/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 409/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6933\n",
      "Epoch 410/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 411/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 412/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 413/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 414/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 415/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 416/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6949\n",
      "Epoch 417/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 418/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 419/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 420/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 421/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 422/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6944\n",
      "Epoch 423/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 424/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 425/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 426/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 427/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6951\n",
      "Epoch 428/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 429/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 430/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 431/500\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.6935\n",
      "Epoch 432/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 433/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 434/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 435/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6959\n",
      "Epoch 436/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 437/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 438/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6946\n",
      "Epoch 439/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 440/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 441/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 442/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 443/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 444/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6953\n",
      "Epoch 445/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6955\n",
      "Epoch 446/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6948\n",
      "Epoch 447/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 448/500\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.6944\n",
      "Epoch 449/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6933\n",
      "Epoch 450/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 451/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6947\n",
      "Epoch 452/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 453/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 454/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 455/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 456/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 457/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 458/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 459/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6952\n",
      "Epoch 460/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 461/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 462/500\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.6941\n",
      "Epoch 463/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 464/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 465/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 466/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 467/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 468/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 469/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 470/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 471/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 472/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6944\n",
      "Epoch 473/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 474/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 475/500\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 476/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 477/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 478/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 479/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 480/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 481/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 482/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 483/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6933\n",
      "Epoch 484/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 485/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6944\n",
      "Epoch 486/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 487/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6951\n",
      "Epoch 488/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 489/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 490/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6945\n",
      "Epoch 491/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6945\n",
      "Epoch 492/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 493/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 494/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 495/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 496/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6933\n",
      "Epoch 497/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 498/500\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 499/500\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 500/500\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.6942\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
       "\tmodel=&lt;keras.engine.sequential.Sequential object at 0x7fbe0e7fdcf0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=42\n",
       "\toptimizer=adam\n",
       "\tloss=binary_crossentropy\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=500\n",
       "\toptimizer__learning_rate=0.03\n",
       "\tclass_weight=None\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=&lt;keras.engine.sequential.Sequential object at 0x7fbe0e7fdcf0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=42\n",
       "\toptimizer=adam\n",
       "\tloss=binary_crossentropy\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=500\n",
       "\toptimizer__learning_rate=0.03\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasClassifier(\n",
       "\tmodel=<keras.engine.sequential.Sequential object at 0x7fbe0e7fdcf0>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=42\n",
       "\toptimizer=adam\n",
       "\tloss=binary_crossentropy\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=500\n",
       "\toptimizer__learning_rate=0.03\n",
       "\tclass_weight=None\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model to the data\n",
    "neural_net_wrapped.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 1s 6ms/step\n",
      "51/51 [==============================] - 1s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Getting training scores\n",
    "train_pred = neural_net_wrapped.predict(features_preprocessed)\n",
    "train_probs = neural_net_wrapped.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Deep Neural Network - No Dropout')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://9d409c26f2894e69ad719541b1821f84/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://9d409c26f2894e69ad719541b1821f84/assets\n",
      "2023-03-15 17:55:31.326177: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://2f828d832fea4a238a8e1ae494f6050c: INVALID_ARGUMENT: ram://2f828d832fea4a238a8e1ae494f6050c is a directory.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://9a258246ebf54954b7fb33dd40281c71/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://9a258246ebf54954b7fb33dd40281c71/assets\n",
      "2023-03-15 17:55:41.524279: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://0ea949435a444ee5b1754df1b720038c: INVALID_ARGUMENT: ram://0ea949435a444ee5b1754df1b720038c is a directory.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3b32421a84304e59b0c5f15fcf326713/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3b32421a84304e59b0c5f15fcf326713/assets\n",
      "2023-03-15 17:55:51.646426: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://c5f3e152f17b4f288a9b42015b7945df: INVALID_ARGUMENT: ram://c5f3e152f17b4f288a9b42015b7945df is a directory.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://fa070b03b7de44f9a24968fb43654ade/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://fa070b03b7de44f9a24968fb43654ade/assets\n",
      "2023-03-15 17:56:00.979485: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://60223ccbfc5142b2b0d87d46ff9f7727: INVALID_ARGUMENT: ram://60223ccbfc5142b2b0d87d46ff9f7727 is a directory.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://a19e932387064462b30e016196e1536b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://a19e932387064462b30e016196e1536b/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://767a58d3177f4f149bee9c2ac6573fcf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://767a58d3177f4f149bee9c2ac6573fcf/assets\n",
      "2023-03-15 17:56:13.810096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "2023-03-15 17:56:16.980032: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://bdb2c9a9ed7e4cd787739b033edf67f3: INVALID_ARGUMENT: ram://bdb2c9a9ed7e4cd787739b033edf67f3 is a directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://81c3232a184040f889fdde5b0826fbf7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://81c3232a184040f889fdde5b0826fbf7/assets\n",
      "2023-03-15 17:56:24.879082: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3313bcd24cd1418990a60763f8341844/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3313bcd24cd1418990a60763f8341844/assets\n",
      "2023-03-15 17:56:31.584648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://bec6d5e4b0e546039169ba2c16a40527/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://bec6d5e4b0e546039169ba2c16a40527/assets\n",
      "2023-03-15 17:56:39.296448: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 17:56:39.357737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "2023-03-15 17:56:44.722077: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://de06b84af80b4a67b78d428ddc8363a1: INVALID_ARGUMENT: ram://de06b84af80b4a67b78d428ddc8363a1 is a directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "INFO:tensorflow:Assets written to: ram://6b13e9b2534d4ec2882d364de12fa440/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6b13e9b2534d4ec2882d364de12fa440/assets\n",
      "2023-03-15 17:56:48.112476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 6s 8ms/step - loss: 0.6934\n",
      "Epoch 2/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 3/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 4/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 5/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 6/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 7/500\n",
      "28/41 [===================>..........] - ETA: 0s - loss: 0.6939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 17:56:53.803185: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://9cabb6b9d4654ff19ae171b759025157: INVALID_ARGUMENT: ram://9cabb6b9d4654ff19ae171b759025157 is a directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 8/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 9/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 10/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 11/500\n",
      "26/41 [==================>...........] - ETA: 0s - loss: 0.6935Epoch 1/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 12/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 13/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 14/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 15/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 16/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6945\n",
      "Epoch 17/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 18/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 19/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6934\n",
      "Epoch 20/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6933\n",
      "Epoch 21/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 22/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6941\n",
      "Epoch 23/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6943\n",
      "Epoch 24/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6948\n",
      "Epoch 25/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 26/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 27/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 28/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 29/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 30/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 31/500\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6930"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 17:57:01.242071: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 32/500\n",
      "41/41 [==============================] - 7s 11ms/step - loss: 0.6936\n",
      "Epoch 2/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6933\n",
      "Epoch 33/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 3/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6952\n",
      "Epoch 34/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "36/41 [=========================>....] - ETA: 0s - loss: 0.6934Epoch 35/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "Epoch 4/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 5/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6936\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6944Epoch 36/500\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 0.6936\n",
      "Epoch 37/500\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.6937\n",
      "Epoch 6/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 38/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6942\n",
      "Epoch 7/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 39/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6952\n",
      "Epoch 8/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6937\n",
      "Epoch 40/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6996Epoch 9/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      "Epoch 41/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6933\n",
      "Epoch 10/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6941\n",
      "Epoch 11/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 42/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 43/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 12/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 44/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 13/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 45/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6941\n",
      "Epoch 14/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 46/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6944\n",
      "Epoch 15/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      "Epoch 47/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      "Epoch 16/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6940\n",
      "Epoch 48/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6935\n",
      "Epoch 17/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.6935Epoch 49/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6936\n",
      "Epoch 18/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "Epoch 50/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 19/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6935\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.6937Epoch 51/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 20/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 52/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6945\n",
      "Epoch 21/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6945\n",
      "Epoch 22/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6942\n",
      "Epoch 53/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6941\n",
      "Epoch 54/500\n",
      "41/41 [==============================] - 1s 11ms/step - loss: 0.6945\n",
      "Epoch 23/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6944\n",
      "Epoch 55/500\n",
      "36/41 [=========================>....] - ETA: 0s - loss: 0.6943"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 17:57:12.369352: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://e483d93094d54fbf99def1c503868aaa: INVALID_ARGUMENT: ram://e483d93094d54fbf99def1c503868aaa is a directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 24/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6972\n",
      "Epoch 56/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6940\n",
      "Epoch 25/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6951\n",
      "Epoch 57/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 26/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "32/41 [======================>.......] - ETA: 0s - loss: 0.6943Epoch 58/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6942\n",
      "Epoch 27/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 59/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 28/500\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6943Epoch 1/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6946\n",
      "Epoch 60/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6945\n",
      "Epoch 29/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 61/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 30/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "25/41 [=================>............] - ETA: 0s - loss: 0.6939Epoch 62/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 31/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 63/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 32/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6935\n",
      "Epoch 64/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "Epoch 33/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6935\n",
      "Epoch 65/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6943\n",
      "Epoch 34/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 66/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 35/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 67/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      "Epoch 36/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 68/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6934\n",
      "Epoch 37/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6943\n",
      "Epoch 69/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6937\n",
      "Epoch 38/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 70/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 39/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 71/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 40/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 72/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 41/500\n",
      "23/41 [===============>..............] - ETA: 0s - loss: 0.6924"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 17:57:20.345120: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 73/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "Epoch 42/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6937\n",
      "Epoch 74/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 43/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      "Epoch 75/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6944\n",
      "Epoch 44/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 76/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 45/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6939\n",
      "Epoch 77/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6944\n",
      "Epoch 46/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 78/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 47/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 79/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 48/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 80/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 49/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6953\n",
      "Epoch 81/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 50/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 82/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6942\n",
      "Epoch 51/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 83/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 52/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 84/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 53/500\n",
      "41/41 [==============================] - 11s 17ms/step - loss: 0.6938\n",
      "Epoch 2/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6937Epoch 85/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 54/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 3/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 86/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 55/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 4/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 87/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6945\n",
      "Epoch 56/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 5/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 88/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6938\n",
      "Epoch 57/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6940\n",
      "Epoch 6/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6942\n",
      "Epoch 89/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 58/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 7/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 90/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 59/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6941\n",
      "Epoch 8/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6949Epoch 91/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6955\n",
      "Epoch 60/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6945\n",
      "Epoch 9/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6944\n",
      "Epoch 92/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6932\n",
      "Epoch 61/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 10/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 93/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "20/41 [=============>................] - ETA: 0s - loss: 0.6946Epoch 62/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6941\n",
      "Epoch 11/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6950\n",
      "Epoch 94/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 63/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 12/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 95/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 64/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 13/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 96/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 65/500\n",
      "14/41 [=========>....................] - ETA: 0s - loss: 0.6931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 17:57:32.608315: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://6728f75848094c5e84005dc1dd24ff93: INVALID_ARGUMENT: ram://6728f75848094c5e84005dc1dd24ff93 is a directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6936\n",
      "Epoch 14/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6933\n",
      "Epoch 97/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 66/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 15/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6941\n",
      "Epoch 98/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6961\n",
      "Epoch 67/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 16/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 99/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6958\n",
      "Epoch 68/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 17/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6944\n",
      "Epoch 100/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 69/500\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6949Epoch 1/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 18/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 101/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 70/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "17/41 [===========>..................] - ETA: 0s - loss: 0.6926Epoch 19/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 102/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 71/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 20/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 103/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6954\n",
      "Epoch 72/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 21/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "Epoch 104/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6941\n",
      "Epoch 73/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6949\n",
      "Epoch 22/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 105/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 74/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 23/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6942\n",
      "Epoch 106/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 75/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 24/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6946\n",
      "Epoch 107/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6937\n",
      "Epoch 76/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 25/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6950\n",
      "Epoch 108/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6944\n",
      "Epoch 77/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6945\n",
      "Epoch 26/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6929\n",
      "Epoch 109/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "14/41 [=========>....................] - ETA: 0s - loss: 0.6929Epoch 78/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 27/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6947Epoch 110/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6941\n",
      "Epoch 79/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 28/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 111/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 80/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6939\n",
      "Epoch 29/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 112/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6946\n",
      " 4/41 [=>............................] - ETA: 0s - loss: 0.6933Epoch 81/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 30/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      " 6/41 [===>..........................] - ETA: 0s - loss: 0.6940Epoch 113/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 82/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 31/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 114/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 83/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 32/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 115/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 84/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 33/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 116/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6939\n",
      "Epoch 85/500\n",
      "41/41 [==============================] - 1s 11ms/step - loss: 0.6948\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.6928Epoch 34/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6945\n",
      "Epoch 117/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6932\n",
      "Epoch 86/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6945\n",
      "Epoch 35/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 118/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 87/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6933\n",
      "Epoch 36/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 119/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6934\n",
      "Epoch 88/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 37/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6936\n",
      "Epoch 120/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 89/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 38/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6933\n",
      "Epoch 121/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6950\n",
      "Epoch 90/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 39/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 122/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 91/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6936\n",
      "Epoch 40/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 123/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6941\n",
      "Epoch 92/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 41/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 124/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 93/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 42/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 125/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 94/500\n",
      "41/41 [==============================] - 15s 14ms/step - loss: 0.6944\n",
      "Epoch 2/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "Epoch 43/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 126/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      " 6/41 [===>..........................] - ETA: 1s - loss: 0.6936Epoch 95/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6942\n",
      "11/41 [=======>......................] - ETA: 0s - loss: 0.6943Epoch 3/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6938\n",
      "Epoch 44/500\n",
      "41/41 [==============================] - 1s 20ms/step - loss: 0.6940\n",
      "Epoch 127/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6943\n",
      "Epoch 96/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 4/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 45/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 128/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 97/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6942\n",
      "Epoch 5/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6936\n",
      "Epoch 46/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 129/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6933\n",
      "Epoch 98/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6938\n",
      "Epoch 6/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 47/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6948\n",
      "Epoch 130/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 99/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6942\n",
      "Epoch 7/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 48/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 131/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6946\n",
      "Epoch 100/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 8/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6938\n",
      "Epoch 49/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "25/41 [=================>............] - ETA: 0s - loss: 0.6937Epoch 132/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6945\n",
      "33/41 [=======================>......] - ETA: 0s - loss: 0.6934Epoch 101/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6937\n",
      "Epoch 9/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6936\n",
      "Epoch 50/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6940\n",
      "Epoch 133/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6939\n",
      "Epoch 102/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6937\n",
      "Epoch 10/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 51/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "Epoch 134/500\n",
      "41/41 [==============================] - 1s 20ms/step - loss: 0.6937\n",
      "Epoch 103/500\n",
      "41/41 [==============================] - 1s 20ms/step - loss: 0.6942\n",
      "Epoch 11/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 135/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6934\n",
      "Epoch 52/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6948\n",
      "Epoch 104/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "24/41 [================>.............] - ETA: 0s - loss: 0.6941Epoch 12/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "18/41 [============>.................] - ETA: 0s - loss: 0.6937Epoch 53/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 136/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 105/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 13/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6939\n",
      "Epoch 54/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6940\n",
      "Epoch 137/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6941\n",
      "Epoch 106/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6935\n",
      "Epoch 14/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6934\n",
      "Epoch 55/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6934\n",
      "Epoch 138/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6937\n",
      "Epoch 107/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6937\n",
      "Epoch 15/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 56/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 139/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "16/41 [==========>...................] - ETA: 0s - loss: 0.6952Epoch 108/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 16/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "25/41 [=================>............] - ETA: 0s - loss: 0.6940Epoch 57/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 140/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "13/41 [========>.....................] - ETA: 0s - loss: 0.6934Epoch 109/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6933\n",
      "Epoch 17/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 58/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "Epoch 141/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6950\n",
      "Epoch 110/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 18/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 59/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6936\n",
      "Epoch 142/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "Epoch 111/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6961Epoch 19/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6956\n",
      "Epoch 60/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6949\n",
      "Epoch 143/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "24/41 [================>.............] - ETA: 0s - loss: 0.6936Epoch 112/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6937\n",
      "Epoch 20/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6937\n",
      "Epoch 61/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6954\n",
      "39/41 [===========================>..] - ETA: 0s - loss: 0.6934Epoch 144/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6933\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6953Epoch 113/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 21/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 62/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 145/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6938\n",
      "Epoch 114/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6955\n",
      "13/41 [========>.....................] - ETA: 0s - loss: 0.6941Epoch 22/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6945\n",
      "Epoch 63/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 146/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 115/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 23/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6946\n",
      "Epoch 64/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 147/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6962\n",
      "Epoch 116/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 24/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 65/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "27/41 [==================>...........] - ETA: 0s - loss: 0.6933Epoch 148/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6949\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6938Epoch 117/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 25/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 66/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6943\n",
      "Epoch 149/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6946\n",
      "15/41 [=========>....................] - ETA: 0s - loss: 0.6900Epoch 118/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "13/41 [========>.....................] - ETA: 0s - loss: 0.6933Epoch 26/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 67/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6973Epoch 119/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6936\n",
      "Epoch 150/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 27/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6945\n",
      "Epoch 68/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 151/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 120/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 28/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 69/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 121/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "12/41 [=======>......................] - ETA: 0s - loss: 0.6916Epoch 152/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 29/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6938\n",
      "Epoch 70/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6938\n",
      "Epoch 122/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6944\n",
      "Epoch 153/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6937\n",
      "Epoch 30/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "Epoch 71/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 123/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 154/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      " 6/41 [===>..........................] - ETA: 1s - loss: 0.6953Epoch 31/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 72/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 124/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6935Epoch 155/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 32/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 73/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 125/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 156/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 33/500\n",
      "41/41 [==============================] - 1s 20ms/step - loss: 0.6936\n",
      "Epoch 74/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "Epoch 126/500\n",
      "41/41 [==============================] - 1s 20ms/step - loss: 0.6936\n",
      "Epoch 157/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6937\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6946Epoch 34/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 75/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6938\n",
      "Epoch 127/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "16/41 [==========>...................] - ETA: 0s - loss: 0.6927Epoch 158/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6948\n",
      "Epoch 35/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 76/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6947\n",
      "Epoch 128/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 159/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 36/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6948\n",
      "Epoch 77/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6936\n",
      "Epoch 129/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6933\n",
      "Epoch 160/500\n",
      "41/41 [==============================] - 1s 20ms/step - loss: 0.6940\n",
      "Epoch 37/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.6935Epoch 130/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "33/41 [=======================>......] - ETA: 0s - loss: 0.6936Epoch 78/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 161/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 38/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 131/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 79/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 162/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6939\n",
      "Epoch 39/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 80/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6949\n",
      "Epoch 163/500\n",
      " 5/41 [==>...........................] - ETA: 0s - loss: 0.6920Epoch 132/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 40/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6944\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.6938Epoch 81/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 133/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6942\n",
      "Epoch 164/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 41/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 82/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6940\n",
      "Epoch 134/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6941\n",
      "Epoch 165/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6941\n",
      "Epoch 42/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6939\n",
      "Epoch 83/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 135/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 166/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 43/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6936\n",
      "Epoch 84/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "Epoch 136/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6937\n",
      "Epoch 167/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6937\n",
      "Epoch 44/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6937\n",
      "Epoch 85/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6940\n",
      "Epoch 137/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6937\n",
      "Epoch 168/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6942\n",
      "Epoch 45/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "Epoch 86/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "Epoch 138/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6938\n",
      "Epoch 169/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "12/41 [=======>......................] - ETA: 0s - loss: 0.6931Epoch 46/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6944\n",
      "Epoch 87/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 139/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 170/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 47/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "26/41 [==================>...........] - ETA: 0s - loss: 0.6947Epoch 88/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6948Epoch 140/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6944\n",
      "Epoch 48/500\n",
      "41/41 [==============================] - 1s 20ms/step - loss: 0.6946\n",
      "Epoch 171/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6944\n",
      "Epoch 141/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 89/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 172/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6949\n",
      "Epoch 49/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 142/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6939\n",
      "Epoch 90/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6941\n",
      "Epoch 173/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "Epoch 50/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6933\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6948Epoch 91/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6953\n",
      "Epoch 143/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 174/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6947\n",
      "Epoch 51/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6939\n",
      "18/41 [============>.................] - ETA: 0s - loss: 0.6939Epoch 92/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6946\n",
      "Epoch 144/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6939\n",
      "Epoch 175/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6938\n",
      "Epoch 52/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 93/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6942\n",
      "Epoch 145/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 176/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 53/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "21/41 [==============>...............] - ETA: 0s - loss: 0.6959Epoch 94/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 146/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6951\n",
      "Epoch 177/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.6929Epoch 54/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 95/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 147/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "Epoch 178/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 55/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 96/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 148/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 179/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 56/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6938\n",
      "Epoch 97/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 149/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 180/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 57/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 98/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 150/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "24/41 [================>.............] - ETA: 0s - loss: 0.6935Epoch 181/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "Epoch 58/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6947\n",
      "Epoch 99/500\n",
      " 2/41 [>.............................] - ETA: 2s - loss: 0.6883Epoch 151/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6942\n",
      "Epoch 182/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6954\n",
      "Epoch 59/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 152/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6947\n",
      "Epoch 100/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6949\n",
      "Epoch 183/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6943\n",
      "Epoch 60/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 101/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6943\n",
      "Epoch 153/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 184/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 61/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6944\n",
      "Epoch 102/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "Epoch 154/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6942\n",
      "Epoch 185/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 62/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 103/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 155/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 186/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 63/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6957\n",
      "Epoch 104/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 156/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 187/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 64/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.6938Epoch 105/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 157/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 188/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 65/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6956\n",
      "Epoch 106/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 158/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 189/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 66/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6931\n",
      "Epoch 107/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 159/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 190/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 67/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6942\n",
      "Epoch 108/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 160/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6933\n",
      "Epoch 191/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      " 8/41 [====>.........................] - ETA: 0s - loss: 0.6931Epoch 68/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 109/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 161/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 192/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 69/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6948\n",
      "37/41 [==========================>...] - ETA: 0s - loss: 0.6962Epoch 110/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6959\n",
      "Epoch 162/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 193/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 70/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 111/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 163/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 194/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 71/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "20/41 [=============>................] - ETA: 0s - loss: 0.6946Epoch 112/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 164/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 195/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "11/41 [=======>......................] - ETA: 0s - loss: 0.6934Epoch 72/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 113/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 165/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6937Epoch 196/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 73/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6927Epoch 114/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 166/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 197/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 74/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6949\n",
      "Epoch 115/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 167/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 198/500\n",
      "41/41 [==============================] - 1s 11ms/step - loss: 0.6947\n",
      "Epoch 75/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6956\n",
      "Epoch 116/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 168/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 199/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 76/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "Epoch 117/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 169/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 200/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6947\n",
      "Epoch 77/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6954\n",
      "Epoch 118/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 170/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 201/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 78/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 119/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6944\n",
      "Epoch 171/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6941\n",
      "Epoch 202/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 79/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 120/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 172/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6938\n",
      "Epoch 203/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 80/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "24/41 [================>.............] - ETA: 0s - loss: 0.6944Epoch 121/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 173/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6946\n",
      "Epoch 204/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 81/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 122/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6946\n",
      "Epoch 174/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6955\n",
      " 5/41 [==>...........................] - ETA: 0s - loss: 0.6937Epoch 205/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 82/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 123/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 206/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 175/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 83/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 124/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 176/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      " 9/41 [=====>........................] - ETA: 0s - loss: 0.6917Epoch 207/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 84/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6934\n",
      "Epoch 125/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 208/500\n",
      "Epoch 177/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6937\n",
      "Epoch 85/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 126/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6952\n",
      "Epoch 209/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 178/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 86/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "35/41 [========================>.....] - ETA: 0s - loss: 0.6952Epoch 127/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6950\n",
      "Epoch 210/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 179/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 87/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 128/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 211/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      " 3/41 [=>............................] - ETA: 0s - loss: 0.6970Epoch 180/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "Epoch 88/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 129/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 212/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 181/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "14/41 [=========>....................] - ETA: 0s - loss: 0.6932Epoch 89/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "32/41 [======================>.......] - ETA: 0s - loss: 0.6932Epoch 130/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 213/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 182/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 90/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 131/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 214/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 183/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 91/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6948\n",
      "Epoch 215/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 132/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 184/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 92/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6933\n",
      "Epoch 216/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6946\n",
      "Epoch 133/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 185/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 93/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "30/41 [====================>.........] - ETA: 0s - loss: 0.6947Epoch 134/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6952\n",
      "Epoch 217/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 186/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 94/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 135/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "Epoch 218/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 187/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 95/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 136/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      " 3/41 [=>............................] - ETA: 1s - loss: 0.6932Epoch 219/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6943\n",
      "Epoch 188/500\n",
      "Epoch 96/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6941\n",
      "Epoch 137/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6984Epoch 220/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 97/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 189/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 138/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6936Epoch 221/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6946\n",
      "Epoch 98/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 190/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 139/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6932Epoch 222/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 99/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 191/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 140/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 223/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 192/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6950\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6909Epoch 100/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6947\n",
      "Epoch 141/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "Epoch 224/500\n",
      " 5/41 [==>...........................] - ETA: 0s - loss: 0.6963Epoch 193/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 101/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 225/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 142/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6959\n",
      "Epoch 194/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6943\n",
      "Epoch 102/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 143/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 195/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 226/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 103/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 144/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 227/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 196/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "Epoch 104/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6935Epoch 145/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 197/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 228/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "12/41 [=======>......................] - ETA: 0s - loss: 0.6961Epoch 105/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "35/41 [========================>.....] - ETA: 0s - loss: 0.6933Epoch 146/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 198/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 229/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6969\n",
      "Epoch 106/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 147/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 199/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      " 9/41 [=====>........................] - ETA: 0s - loss: 0.6927Epoch 230/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 107/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6933\n",
      "Epoch 148/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6948\n",
      "Epoch 200/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 108/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6952Epoch 231/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 149/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6949\n",
      "29/41 [====================>.........] - ETA: 0s - loss: 0.6935Epoch 201/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 232/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6914Epoch 109/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 150/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 202/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 233/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6948\n",
      "Epoch 110/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 151/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 234/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6946\n",
      "Epoch 203/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 111/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 152/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 235/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6955\n",
      "Epoch 204/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6951\n",
      "Epoch 112/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6944\n",
      "32/41 [======================>.......] - ETA: 0s - loss: 0.6962Epoch 153/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 236/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6958\n",
      "Epoch 205/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "11/41 [=======>......................] - ETA: 0s - loss: 0.6939Epoch 113/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 154/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 237/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6937\n",
      "Epoch 206/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "14/41 [=========>....................] - ETA: 0s - loss: 0.6941Epoch 114/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6944\n",
      "Epoch 155/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 207/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 238/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "16/41 [==========>...................] - ETA: 0s - loss: 0.6938Epoch 115/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 156/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 239/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 208/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6950\n",
      "Epoch 116/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "Epoch 157/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 240/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.6940Epoch 209/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "10/41 [======>.......................] - ETA: 0s - loss: 0.6934Epoch 117/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 158/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6933\n",
      "Epoch 241/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 210/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6951\n",
      "Epoch 118/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 159/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6946\n",
      "Epoch 242/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 211/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 119/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "Epoch 160/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 243/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 120/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6934\n",
      "Epoch 212/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "26/41 [==================>...........] - ETA: 0s - loss: 0.6937Epoch 161/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 244/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 213/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 121/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6959\n",
      "Epoch 162/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 214/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 245/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6932\n",
      "11/41 [=======>......................] - ETA: 0s - loss: 0.6931Epoch 122/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 163/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6940\n",
      "Epoch 246/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 215/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 123/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 164/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 247/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 216/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 124/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6946\n",
      "Epoch 165/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 248/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 217/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6937\n",
      " 8/41 [====>.........................] - ETA: 0s - loss: 0.6937Epoch 125/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6935\n",
      "Epoch 166/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6934\n",
      "Epoch 249/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 218/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 126/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 167/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 219/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6946\n",
      "Epoch 250/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 127/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 168/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6950\n",
      "Epoch 251/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6941\n",
      "Epoch 220/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6941\n",
      "Epoch 128/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 169/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6949Epoch 252/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "Epoch 221/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6947\n",
      "Epoch 129/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 170/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6944\n",
      "Epoch 222/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6949\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6923Epoch 253/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 130/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 171/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 223/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6946\n",
      "Epoch 254/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6936Epoch 131/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6956\n",
      "Epoch 172/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.6941Epoch 224/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6941\n",
      "Epoch 255/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6948\n",
      "Epoch 132/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 173/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 225/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 256/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6944\n",
      "Epoch 133/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 174/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6948\n",
      "Epoch 226/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 257/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "12/41 [=======>......................] - ETA: 0s - loss: 0.6936Epoch 134/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "28/41 [===================>..........] - ETA: 0s - loss: 0.6933Epoch 175/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6932\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6935Epoch 227/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 258/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6934\n",
      "Epoch 135/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 176/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      " 8/41 [====>.........................] - ETA: 0s - loss: 0.6935Epoch 228/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6932Epoch 259/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 136/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "33/41 [=======================>......] - ETA: 0s - loss: 0.6940Epoch 177/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 260/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6955\n",
      "Epoch 229/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 137/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 178/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 261/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 230/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 138/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 179/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 231/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6933\n",
      "Epoch 262/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 139/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "30/41 [====================>.........] - ETA: 0s - loss: 0.6936Epoch 180/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 232/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 263/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 140/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 181/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 264/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6941\n",
      "Epoch 233/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 141/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6935\n",
      "Epoch 182/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 265/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6934\n",
      "Epoch 234/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 142/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "23/41 [===============>..............] - ETA: 0s - loss: 0.6934Epoch 183/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6942\n",
      "Epoch 266/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 235/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "14/41 [=========>....................] - ETA: 0s - loss: 0.6929Epoch 143/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "33/41 [=======================>......] - ETA: 0s - loss: 0.6944Epoch 184/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 236/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 267/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6940\n",
      "23/41 [===============>..............] - ETA: 0s - loss: 0.6936Epoch 144/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 185/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 268/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "32/41 [======================>.......] - ETA: 0s - loss: 0.6937Epoch 237/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "Epoch 145/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 186/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 238/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 269/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 146/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 187/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6940\n",
      "Epoch 270/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 239/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6953\n",
      "21/41 [==============>...............] - ETA: 0s - loss: 0.6939Epoch 147/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6943\n",
      "Epoch 188/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 271/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 240/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 148/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 189/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6943\n",
      "Epoch 272/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 241/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 149/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.6942Epoch 190/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 273/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 242/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6953\n",
      "Epoch 150/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 191/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 274/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 243/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 151/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 192/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 275/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6935Epoch 244/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 152/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 193/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 276/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 245/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 153/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6948\n",
      "Epoch 194/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 277/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 246/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6956\n",
      "Epoch 154/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6942\n",
      "Epoch 195/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 278/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6938\n",
      "Epoch 247/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6943\n",
      "Epoch 155/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 196/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 279/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 248/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 156/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 197/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 249/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6942\n",
      "Epoch 280/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 157/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 198/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 281/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6972\n",
      "Epoch 250/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 158/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6941\n",
      "Epoch 199/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "28/41 [===================>..........] - ETA: 0s - loss: 0.6937Epoch 282/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6940\n",
      "Epoch 251/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 159/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 200/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 283/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "35/41 [========================>.....] - ETA: 0s - loss: 0.6933Epoch 252/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 160/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6953\n",
      "Epoch 201/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 253/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 284/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 161/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 202/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "32/41 [======================>.......] - ETA: 0s - loss: 0.6949Epoch 285/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6946\n",
      "Epoch 162/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 254/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 203/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 286/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 255/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 163/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6948\n",
      "Epoch 204/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 256/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6948\n",
      "Epoch 287/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      " 8/41 [====>.........................] - ETA: 0s - loss: 0.6933Epoch 164/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6955\n",
      "Epoch 205/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6953\n",
      "Epoch 288/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 165/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 257/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "30/41 [====================>.........] - ETA: 0s - loss: 0.6942Epoch 206/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 289/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 258/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "14/41 [=========>....................] - ETA: 0s - loss: 0.6938Epoch 166/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "Epoch 207/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 290/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 167/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 259/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "24/41 [================>.............] - ETA: 0s - loss: 0.6938Epoch 208/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 291/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 168/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 260/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 209/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "35/41 [========================>.....] - ETA: 0s - loss: 0.6949Epoch 292/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "Epoch 169/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 261/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 210/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 293/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 262/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 170/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 211/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 294/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      " 8/41 [====>.........................] - ETA: 0s - loss: 0.6938Epoch 263/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 171/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "Epoch 295/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 212/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 264/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 172/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 296/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6942Epoch 213/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 173/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      " 9/41 [=====>........................] - ETA: 0s - loss: 0.6942Epoch 265/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 297/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "Epoch 214/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 266/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6946\n",
      "Epoch 174/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 298/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "33/41 [=======================>......] - ETA: 0s - loss: 0.6934Epoch 215/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 267/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6933\n",
      "Epoch 175/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 299/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "Epoch 216/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 176/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6944\n",
      "Epoch 268/500\n",
      "41/41 [==============================] - 1s 9ms/step - loss: 0.6934\n",
      "Epoch 300/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 217/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 177/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6955\n",
      "Epoch 269/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 301/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 218/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 270/500\n",
      "11/41 [=======>......................] - ETA: 0s - loss: 0.6942Epoch 178/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 302/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 219/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 271/500\n",
      "Epoch 179/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6942\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.6937Epoch 303/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 220/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 180/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6933\n",
      "Epoch 272/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 221/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 304/500\n",
      "Epoch 181/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 273/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "39/41 [===========================>..] - ETA: 0s - loss: 0.6934Epoch 222/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 182/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 305/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 274/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6948\n",
      "Epoch 223/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 306/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6953\n",
      "39/41 [===========================>..] - ETA: 0s - loss: 0.6937Epoch 183/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 275/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 224/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      " 1/41 [..............................] - ETA: 4s - loss: 0.6932Epoch 307/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      " 4/41 [=>............................] - ETA: 0s - loss: 0.6919Epoch 276/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 184/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6944\n",
      "Epoch 308/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 225/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 185/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 277/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6937\n",
      "Epoch 309/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 226/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 186/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 278/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6933\n",
      "Epoch 227/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 310/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 187/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 279/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 311/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.6935Epoch 228/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 188/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 280/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6959\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6937Epoch 229/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 312/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 189/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 281/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6948\n",
      "Epoch 230/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 190/500\n",
      "Epoch 313/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 282/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 231/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 314/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 191/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 283/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6939\n",
      "23/41 [===============>..............] - ETA: 0s - loss: 0.6941Epoch 232/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 315/500\n",
      "Epoch 192/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 284/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 233/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 193/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6949\n",
      "Epoch 316/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6942\n",
      "Epoch 285/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 234/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 194/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "16/41 [==========>...................] - ETA: 0s - loss: 0.6935Epoch 317/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 286/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 235/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 195/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 318/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6909Epoch 287/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 236/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 196/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 319/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 288/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 237/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 197/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 320/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 289/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 238/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 198/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 321/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 290/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6932\n",
      "Epoch 239/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.6936Epoch 199/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 322/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 291/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 240/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 323/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 200/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6943\n",
      "Epoch 292/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 241/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6956\n",
      "Epoch 201/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6946\n",
      "Epoch 293/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 324/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 242/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 202/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 325/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6911Epoch 294/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 243/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 203/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 326/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6932Epoch 295/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6941Epoch 244/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 327/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 204/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 296/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 245/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 328/500\n",
      "Epoch 205/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      " 2/41 [>.............................] - ETA: 3s - loss: 0.6930Epoch 297/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 246/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6942\n",
      "Epoch 298/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 206/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 329/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6941\n",
      "25/41 [=================>............] - ETA: 0s - loss: 0.6943Epoch 247/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 207/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 330/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 299/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6944\n",
      "Epoch 248/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 331/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 208/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "18/41 [============>.................] - ETA: 0s - loss: 0.6939Epoch 300/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 249/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.6936Epoch 209/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.6940Epoch 332/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 301/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6961\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6946Epoch 250/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6944\n",
      "13/41 [========>.....................] - ETA: 0s - loss: 0.6939Epoch 333/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 302/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 210/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 251/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 334/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 303/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 211/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 252/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 335/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6947\n",
      "Epoch 304/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      " 4/41 [=>............................] - ETA: 0s - loss: 0.6953Epoch 212/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6944\n",
      "Epoch 253/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 336/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 305/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 213/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "36/41 [=========================>....] - ETA: 0s - loss: 0.6944Epoch 254/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "33/41 [=======================>......] - ETA: 0s - loss: 0.6960Epoch 337/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6954\n",
      "Epoch 214/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 306/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 255/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 338/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 307/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 215/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6946\n",
      "Epoch 256/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 339/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 216/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6943\n",
      "Epoch 308/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 257/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 340/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 309/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 217/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 258/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6935\n",
      "32/41 [======================>.......] - ETA: 0s - loss: 0.6940Epoch 341/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 310/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 218/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 259/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 342/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 311/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 219/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6944\n",
      "Epoch 260/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 343/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 220/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 312/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6934\n",
      "Epoch 261/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 344/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 221/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6930Epoch 313/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 262/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 345/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 222/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "20/41 [=============>................] - ETA: 0s - loss: 0.6936Epoch 314/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 263/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6947\n",
      "Epoch 346/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6947\n",
      "Epoch 315/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 223/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6933\n",
      "Epoch 264/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "28/41 [===================>..........] - ETA: 0s - loss: 0.6966Epoch 347/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 224/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6964\n",
      "Epoch 316/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 348/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 265/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6954\n",
      "Epoch 317/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 225/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6937\n",
      "Epoch 266/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 349/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "10/41 [======>.......................] - ETA: 0s - loss: 0.6936Epoch 318/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 226/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 267/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6955\n",
      "11/41 [=======>......................] - ETA: 0s - loss: 0.6943Epoch 350/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "Epoch 227/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6947\n",
      "Epoch 319/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 268/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 351/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6944\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 228/500\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6939Epoch 320/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 269/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6934\n",
      "Epoch 352/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6948\n",
      "Epoch 229/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 321/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "21/41 [==============>...............] - ETA: 0s - loss: 0.6934Epoch 270/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 353/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 322/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6952\n",
      "Epoch 230/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 271/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 354/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 323/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6938\n",
      "Epoch 231/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 272/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.6936Epoch 355/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 324/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "18/41 [============>.................] - ETA: 0s - loss: 0.6936Epoch 232/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 273/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 356/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 325/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 233/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 274/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 326/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 357/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 234/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 275/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "37/41 [==========================>...] - ETA: 0s - loss: 0.6936Epoch 358/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 327/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      " 8/41 [====>.........................] - ETA: 0s - loss: 0.6928Epoch 235/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 276/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 359/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 328/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6946\n",
      "Epoch 236/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 277/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 360/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 329/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 237/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 278/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 330/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 361/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 238/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 279/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6937\n",
      "Epoch 362/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6954\n",
      "Epoch 331/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6931\n",
      "28/41 [===================>..........] - ETA: 0s - loss: 0.6935Epoch 239/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 280/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 363/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 332/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 240/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 281/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 364/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 333/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6943\n",
      "28/41 [===================>..........] - ETA: 0s - loss: 0.6932Epoch 241/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "20/41 [=============>................] - ETA: 0s - loss: 0.6929Epoch 282/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 365/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 334/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6947\n",
      "Epoch 242/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 283/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 366/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6944\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 243/500\n",
      "Epoch 335/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 284/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6956\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6937Epoch 367/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "Epoch 336/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6930Epoch 244/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 285/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 368/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 245/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6964\n",
      "Epoch 337/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "13/41 [========>.....................] - ETA: 0s - loss: 0.6932Epoch 286/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 369/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "26/41 [==================>...........] - ETA: 0s - loss: 0.6935Epoch 246/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 338/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 287/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6935\n",
      "Epoch 370/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 247/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6946Epoch 339/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 288/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "29/41 [====================>.........] - ETA: 0s - loss: 0.6938Epoch 371/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 248/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 340/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 289/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 372/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 249/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 341/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 290/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 373/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6962\n",
      "Epoch 250/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 342/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 291/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6952\n",
      "Epoch 374/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 343/500\n",
      "Epoch 251/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6943\n",
      "Epoch 292/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6950\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6943Epoch 375/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 252/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6937\n",
      " 3/41 [=>............................] - ETA: 1s - loss: 0.6937Epoch 344/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 293/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 376/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6942\n",
      "Epoch 345/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 253/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 294/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6939\n",
      "Epoch 377/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 254/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 346/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6942\n",
      "Epoch 295/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 378/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 347/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "Epoch 255/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6948\n",
      "Epoch 296/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "Epoch 379/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 348/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6944\n",
      "Epoch 256/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 297/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 380/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 349/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 257/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "15/41 [=========>....................] - ETA: 0s - loss: 0.6952Epoch 298/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6959\n",
      "17/41 [===========>..................] - ETA: 0s - loss: 0.6941Epoch 381/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 350/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 258/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 299/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6944\n",
      "Epoch 382/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6937\n",
      "Epoch 259/500\n",
      "Epoch 351/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 300/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 352/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 383/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 260/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 301/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 353/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 384/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 261/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 302/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 354/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 385/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6933\n",
      "26/41 [==================>...........] - ETA: 0s - loss: 0.6933Epoch 262/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 303/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 355/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 386/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "21/41 [==============>...............] - ETA: 0s - loss: 0.6919Epoch 263/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6943\n",
      "Epoch 304/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6950\n",
      "Epoch 356/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 264/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 387/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 305/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 357/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6937\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 388/500\n",
      "20/41 [=============>................] - ETA: 0s - loss: 0.6935Epoch 265/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 306/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 358/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 266/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6932Epoch 389/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6945\n",
      "Epoch 307/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 359/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 267/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6959Epoch 390/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6952\n",
      "Epoch 308/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 268/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 360/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 391/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6932\n",
      "Epoch 309/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6936\n",
      "Epoch 361/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "16/41 [==========>...................] - ETA: 0s - loss: 0.6945Epoch 269/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.6944Epoch 392/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 310/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 362/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 270/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 393/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 311/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 363/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 271/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6951\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6932Epoch 394/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 312/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 364/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 272/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6959\n",
      "Epoch 395/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "25/41 [=================>............] - ETA: 0s - loss: 0.6928Epoch 313/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "29/41 [====================>.........] - ETA: 0s - loss: 0.6939Epoch 365/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 273/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 396/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 314/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 366/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 274/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6959\n",
      "Epoch 397/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "30/41 [====================>.........] - ETA: 0s - loss: 0.6938Epoch 315/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "26/41 [==================>...........] - ETA: 0s - loss: 0.6961Epoch 367/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 275/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6963\n",
      "Epoch 398/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6954\n",
      "Epoch 316/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 368/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 276/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6951\n",
      "Epoch 399/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 317/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "24/41 [================>.............] - ETA: 0s - loss: 0.6950Epoch 369/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 277/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6945\n",
      "Epoch 400/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "Epoch 318/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6931\n",
      "Epoch 370/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 278/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "Epoch 401/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6943\n",
      "Epoch 319/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6946\n",
      "Epoch 371/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 279/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6932\n",
      "Epoch 402/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6951\n",
      "Epoch 320/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 372/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 280/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 403/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 321/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 373/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6944\n",
      "Epoch 281/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6940\n",
      "Epoch 404/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 322/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 374/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6946\n",
      "Epoch 282/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 405/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 323/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 375/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6942\n",
      "Epoch 283/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 406/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "30/41 [====================>.........] - ETA: 0s - loss: 0.6936Epoch 324/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 376/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 284/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 407/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6938\n",
      "Epoch 325/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 377/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6937\n",
      "Epoch 285/500\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.6954\n",
      "Epoch 408/500\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.6934\n",
      "Epoch 326/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6938\n",
      "Epoch 378/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6947\n",
      "Epoch 286/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 409/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 327/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 379/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 287/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 410/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6942\n",
      "24/41 [================>.............] - ETA: 0s - loss: 0.6933Epoch 328/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 380/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6941\n",
      "Epoch 288/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6938\n",
      "Epoch 411/500\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 0.6937\n",
      "Epoch 329/500\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.6937\n",
      "Epoch 381/500\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 0.6940\n",
      "Epoch 289/500\n",
      "41/41 [==============================] - 1s 29ms/step - loss: 0.6938\n",
      "Epoch 412/500\n",
      "41/41 [==============================] - 1s 24ms/step - loss: 0.6934\n",
      "Epoch 330/500\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.6937\n",
      "Epoch 382/500\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.6936\n",
      "Epoch 290/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 413/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6948\n",
      "Epoch 331/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "Epoch 383/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 291/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 332/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "Epoch 414/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 384/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 292/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 415/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 333/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 385/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "29/41 [====================>.........] - ETA: 0s - loss: 0.6947Epoch 293/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 416/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 334/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6937\n",
      "Epoch 386/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 294/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 417/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6953\n",
      "Epoch 335/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 387/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 295/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 336/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 418/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6940\n",
      "Epoch 388/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6961\n",
      "Epoch 296/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 419/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6948\n",
      "Epoch 337/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "Epoch 389/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "Epoch 297/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 338/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "Epoch 420/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6946\n",
      "Epoch 390/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 298/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 339/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 421/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 391/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 299/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 340/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 422/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6953\n",
      "Epoch 392/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6937\n",
      "Epoch 300/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6935\n",
      "Epoch 341/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 423/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 393/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 301/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6948\n",
      "Epoch 342/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 424/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6934\n",
      "26/41 [==================>...........] - ETA: 0s - loss: 0.6934Epoch 394/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 302/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 425/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 343/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 395/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 303/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 426/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6935\n",
      "Epoch 344/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6941\n",
      "10/41 [======>.......................] - ETA: 0s - loss: 0.6930Epoch 396/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 304/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 427/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 345/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6952\n",
      "Epoch 397/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6936\n",
      "Epoch 305/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 346/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 398/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6896Epoch 428/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 306/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 347/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 429/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6941\n",
      "Epoch 399/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 307/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6941\n",
      "Epoch 348/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 430/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 400/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 308/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6938\n",
      "Epoch 349/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 431/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 401/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6932\n",
      "Epoch 309/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 432/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "Epoch 350/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 402/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 310/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 433/500\n",
      "Epoch 351/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 403/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 311/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6942\n",
      "30/41 [====================>.........] - ETA: 0s - loss: 0.6944Epoch 352/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 404/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6949\n",
      "Epoch 434/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 312/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6946\n",
      "Epoch 353/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6956\n",
      "Epoch 435/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6904Epoch 405/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 313/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 354/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 436/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6952\n",
      "25/41 [=================>............] - ETA: 0s - loss: 0.6935Epoch 406/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 314/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 437/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 407/500\n",
      "Epoch 355/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6954\n",
      "Epoch 315/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 408/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6947\n",
      "Epoch 438/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6944\n",
      " 9/41 [=====>........................] - ETA: 0s - loss: 0.6931Epoch 356/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6949\n",
      "Epoch 316/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6949\n",
      "Epoch 409/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 439/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 357/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 317/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 410/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 358/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 440/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 318/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 411/500\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6940Epoch 441/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      " 6/41 [===>..........................] - ETA: 0s - loss: 0.6913Epoch 359/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6955\n",
      "Epoch 319/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 412/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 442/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 360/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "20/41 [=============>................] - ETA: 0s - loss: 0.6934Epoch 320/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 413/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      " 1/41 [..............................] - ETA: 1s - loss: 0.6944Epoch 443/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 361/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6932\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6938Epoch 321/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 444/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6948\n",
      "Epoch 414/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6956Epoch 362/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 322/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 445/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 415/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 363/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 323/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 446/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 416/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 364/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 324/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 365/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 417/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 447/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 325/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 366/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 448/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6936Epoch 418/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 326/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 449/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.6934Epoch 367/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 419/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 327/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 368/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6934\n",
      "Epoch 450/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 420/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6944\n",
      "Epoch 328/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 369/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 451/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 421/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 329/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 452/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 370/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 422/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 330/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6937\n",
      "Epoch 423/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      "Epoch 371/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 453/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 331/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 424/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6905Epoch 372/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 454/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 332/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 455/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6943\n",
      "Epoch 373/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 425/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "19/41 [============>.................] - ETA: 0s - loss: 0.6941Epoch 333/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 426/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 374/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6940\n",
      "Epoch 456/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.6931Epoch 334/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "16/41 [==========>...................] - ETA: 0s - loss: 0.6931Epoch 427/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6948\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 457/500\n",
      "Epoch 375/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 335/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 428/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6945\n",
      "Epoch 458/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6937\n",
      "Epoch 376/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 336/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 429/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 459/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 377/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6946\n",
      "Epoch 337/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 430/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 460/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 378/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "31/41 [=====================>........] - ETA: 0s - loss: 0.6937Epoch 338/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 431/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 461/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6952\n",
      "Epoch 379/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6953\n",
      "Epoch 339/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 432/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 462/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 380/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 340/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 463/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6962\n",
      "Epoch 433/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 381/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6934\n",
      "26/41 [==================>...........] - ETA: 0s - loss: 0.6928Epoch 341/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 464/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6952\n",
      "Epoch 434/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 382/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 342/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 435/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6935\n",
      "Epoch 383/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6946\n",
      "Epoch 465/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 343/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6924Epoch 436/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "14/41 [=========>....................] - ETA: 0s - loss: 0.6933Epoch 384/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 466/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6938\n",
      "Epoch 344/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6939\n",
      "Epoch 437/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 385/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 467/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 345/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 386/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 468/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 438/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 346/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 469/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 387/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "Epoch 439/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6935\n",
      "Epoch 347/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 440/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 388/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 470/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6955\n",
      "Epoch 348/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 441/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6942\n",
      "Epoch 389/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 471/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 349/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 442/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 390/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 472/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6932\n",
      "Epoch 350/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6940\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 443/500\n",
      "Epoch 391/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 473/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 351/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 474/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6942\n",
      "Epoch 444/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6950\n",
      "Epoch 392/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 475/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6936\n",
      "37/41 [==========================>...] - ETA: 0s - loss: 0.6936Epoch 352/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 445/500\n",
      "Epoch 393/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6939\n",
      "Epoch 476/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6935\n",
      "Epoch 353/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6933\n",
      "Epoch 394/500\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.6943\n",
      "Epoch 446/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6939\n",
      "Epoch 477/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 354/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "21/41 [==============>...............] - ETA: 0s - loss: 0.6953Epoch 447/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 395/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6948\n",
      "Epoch 478/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6942\n",
      " 6/41 [===>..........................] - ETA: 0s - loss: 0.6949Epoch 355/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 448/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 396/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 479/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6946\n",
      "Epoch 356/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6947\n",
      "Epoch 449/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      "Epoch 397/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 480/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6933\n",
      "Epoch 357/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6934Epoch 450/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6945\n",
      "Epoch 398/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 481/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 451/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6944\n",
      "Epoch 358/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 399/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 482/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "Epoch 359/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 452/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6941\n",
      "Epoch 400/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "Epoch 483/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 360/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6943\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6932Epoch 453/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "15/41 [=========>....................] - ETA: 0s - loss: 0.6934Epoch 401/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6943\n",
      "Epoch 484/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 361/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6944\n",
      "Epoch 454/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6953\n",
      "Epoch 402/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "37/41 [==========================>...] - ETA: 0s - loss: 0.6950Epoch 485/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6947\n",
      "Epoch 455/500\n",
      "Epoch 362/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6926Epoch 403/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 486/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6937\n",
      "Epoch 363/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6934\n",
      "Epoch 456/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6938\n",
      "Epoch 404/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 487/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6936\n",
      "Epoch 364/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6950\n",
      "Epoch 457/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6937\n",
      "Epoch 405/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "30/41 [====================>.........] - ETA: 0s - loss: 0.6935Epoch 488/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 365/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 458/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6942\n",
      "Epoch 406/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.6946Epoch 489/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "15/41 [=========>....................] - ETA: 0s - loss: 0.6958Epoch 366/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 459/500\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6910Epoch 407/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6942\n",
      "Epoch 490/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6945\n",
      "Epoch 367/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 460/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6936\n",
      "Epoch 408/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6934\n",
      "Epoch 491/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6937\n",
      "Epoch 461/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 368/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6943\n",
      "Epoch 409/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6936\n",
      "Epoch 492/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6941\n",
      "Epoch 462/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "34/41 [=======================>......] - ETA: 0s - loss: 0.6944Epoch 369/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6942\n",
      "Epoch 410/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6937\n",
      "Epoch 493/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 463/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6933\n",
      "Epoch 370/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 411/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6939\n",
      "Epoch 494/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 464/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 371/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 412/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6934\n",
      "Epoch 495/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 465/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 413/500\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.6938\n",
      "Epoch 372/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6940\n",
      "Epoch 496/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6935\n",
      "33/41 [=======================>......] - ETA: 0s - loss: 0.6950Epoch 466/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6948\n",
      "Epoch 373/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6949\n",
      "Epoch 414/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6953\n",
      "Epoch 467/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6945\n",
      "28/41 [===================>..........] - ETA: 0s - loss: 0.6946Epoch 497/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "Epoch 374/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 415/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 498/500\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6895Epoch 468/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6947\n",
      "Epoch 375/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6938\n",
      "Epoch 416/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6944\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6939\n",
      "Epoch 469/500\n",
      "Epoch 499/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6935\n",
      "Epoch 417/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6940\n",
      "Epoch 376/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6943\n",
      "29/41 [====================>.........] - ETA: 0s - loss: 0.6945Epoch 470/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 500/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6940\n",
      "Epoch 377/500\n",
      "Epoch 418/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6938\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6933\n",
      "Epoch 471/500\n",
      "41/41 [==============================] - 1s 15ms/step - loss: 0.6936\n",
      "Epoch 419/500\n",
      "41/41 [==============================] - 1s 16ms/step - loss: 0.6942\n",
      "Epoch 378/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6942\n",
      "Epoch 472/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6941\n",
      "10/41 [======>.......................] - ETA: 0s - loss: 0.6936Epoch 420/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6943\n",
      "Epoch 379/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 473/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6940\n",
      "Epoch 421/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 380/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6942\n",
      "Epoch 474/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6943\n",
      "Epoch 381/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6934\n",
      "Epoch 422/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6954\n",
      "21/41 [==============>...............] - ETA: 0s - loss: 0.6944Epoch 475/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6940\n",
      "Epoch 382/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6944\n",
      "Epoch 423/500\n",
      "11/11 [==============================] - 2s 9ms/steposs: 0.694\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6945\n",
      "Epoch 476/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6935\n",
      "Epoch 424/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6941\n",
      "Epoch 383/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 477/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6936\n",
      "Epoch 384/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6939\n",
      "Epoch 425/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6943\n",
      "28/41 [===================>..........] - ETA: 0s - loss: 0.6944Epoch 478/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 385/500\n",
      "Epoch 426/500\n",
      "11/11 [==============================] - 0s 5ms/steposs: 0.6813\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6939\n",
      "Epoch 479/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6942\n",
      "Epoch 386/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6937\n",
      "Epoch 427/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "Epoch 480/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 387/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6948\n",
      "12/41 [=======>......................] - ETA: 0s - loss: 0.6943Epoch 428/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6957\n",
      "Epoch 481/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6938\n",
      "Epoch 388/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6938\n",
      "Epoch 429/500\n",
      "16/41 [==========>...................] - ETA: 0s - loss: 0.6948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6944\n",
      "Epoch 482/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6946\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6944\n",
      "Epoch 389/500\n",
      "Epoch 430/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6933\n",
      "29/41 [====================>.........] - ETA: 0s - loss: 0.6934Epoch 483/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6941\n",
      "Epoch 390/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6912Epoch 431/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6960\n",
      "Epoch 484/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6942\n",
      "Epoch 432/500\n",
      "Epoch 391/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6942\n",
      "Epoch 485/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6946\n",
      "Epoch 433/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6941\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6964Epoch 392/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6944\n",
      "Epoch 486/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6956\n",
      "Epoch 434/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 393/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 487/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "10/41 [======>.......................] - ETA: 0s - loss: 0.6938Epoch 435/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 394/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "25/41 [=================>............] - ETA: 0s - loss: 0.6942Epoch 488/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6940\n",
      "14/41 [=========>....................] - ETA: 0s - loss: 0.6934Epoch 436/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6937\n",
      "Epoch 395/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      "Epoch 489/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 437/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6944\n",
      " 5/41 [==>...........................] - ETA: 1s - loss: 0.6940Epoch 396/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6945\n",
      "Epoch 490/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 438/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6951\n",
      "Epoch 397/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6939\n",
      "Epoch 491/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 439/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6943\n",
      "Epoch 398/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 492/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 440/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      " 9/41 [=====>........................] - ETA: 0s - loss: 0.6936Epoch 399/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6945\n",
      "32/41 [======================>.......] - ETA: 0s - loss: 0.6934Epoch 493/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 441/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6936\n",
      "Epoch 400/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6934\n",
      "Epoch 494/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 442/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6941\n",
      "Epoch 401/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6948\n",
      "Epoch 495/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6941\n",
      "25/41 [=================>............] - ETA: 0s - loss: 0.6944Epoch 443/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6942\n",
      "Epoch 402/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6955\n",
      "Epoch 496/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6939\n",
      "Epoch 444/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6942\n",
      "Epoch 403/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6945\n",
      "Epoch 497/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 445/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6940\n",
      "Epoch 404/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 498/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6934\n",
      "Epoch 446/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6933\n",
      "Epoch 405/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6942\n",
      "Epoch 499/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      "Epoch 447/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 406/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "Epoch 500/500\n",
      "41/41 [==============================] - 1s 12ms/step - loss: 0.6935\n",
      "Epoch 448/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6940\n",
      "Epoch 407/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6941\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 449/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 408/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6935\n",
      "Epoch 450/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6944\n",
      "Epoch 409/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 451/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 410/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6935\n",
      "Epoch 452/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      "Epoch 411/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6945\n",
      "Epoch 453/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6944\n",
      "Epoch 412/500\n",
      "11/11 [==============================] - 1s 16ms/stepss: 0.693\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 454/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 413/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 455/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6942\n",
      "Epoch 414/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 456/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 415/500\n",
      "11/11 [==============================] - 0s 3ms/steposs: 0.6973\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6945\n",
      "Epoch 457/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6932Epoch 416/500\n",
      " 9/41 [=====>........................] - ETA: 0s - loss: 0.6925"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 18:01:50.022694: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ram://ff65a19af8a448d68e25f1fef113c3a5: INVALID_ARGUMENT: ram://ff65a19af8a448d68e25f1fef113c3a5 is a directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 458/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 417/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 459/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 418/500\n",
      " 6/41 [===>..........................] - ETA: 0s - loss: 0.6931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 460/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 419/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 461/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6941\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6936Epoch 420/500\n",
      " 8/41 [====>.........................] - ETA: 0s - loss: 0.6934Epoch 1/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 462/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 421/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6933\n",
      "Epoch 463/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 422/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 464/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 423/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 465/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6931Epoch 424/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 466/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 425/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6949\n",
      "Epoch 467/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 426/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 468/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 427/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 469/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 428/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 470/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 429/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 471/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6945\n",
      "Epoch 430/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 472/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 431/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6944\n",
      "Epoch 473/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 432/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 474/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 433/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6946\n",
      "Epoch 475/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 434/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "36/41 [=========================>....] - ETA: 0s - loss: 0.6933Epoch 476/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 435/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 477/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 436/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6948\n",
      "Epoch 478/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 437/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "36/41 [=========================>....] - ETA: 0s - loss: 0.6939Epoch 479/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 438/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 480/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6931\n",
      "Epoch 439/500\n",
      "41/41 [==============================] - 7s 11ms/step - loss: 0.6943\n",
      "Epoch 2/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6953\n",
      "Epoch 481/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 440/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6937\n",
      "Epoch 3/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 482/500\n",
      "41/41 [==============================] - 1s 14ms/step - loss: 0.6937\n",
      "Epoch 441/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "Epoch 4/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6932\n",
      "Epoch 483/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 442/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "Epoch 5/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6948\n",
      "Epoch 484/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      "Epoch 443/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6947\n",
      "Epoch 485/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 6/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6940\n",
      "Epoch 444/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6935\n",
      "Epoch 486/500\n",
      "Epoch 7/500\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 0.6941\n",
      "Epoch 445/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6942\n",
      "Epoch 8/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6934\n",
      "Epoch 487/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 446/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 9/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 488/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "Epoch 447/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6941\n",
      "Epoch 10/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 489/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6944\n",
      "Epoch 448/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 11/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6948\n",
      "Epoch 490/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6941\n",
      "Epoch 449/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6940\n",
      "Epoch 12/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "30/41 [====================>.........] - ETA: 0s - loss: 0.6942Epoch 491/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 450/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 492/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6942\n",
      "Epoch 13/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6942\n",
      "Epoch 451/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6933\n",
      "Epoch 493/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 14/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6936\n",
      "Epoch 452/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.6937Epoch 494/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      " 8/41 [====>.........................] - ETA: 0s - loss: 0.6956Epoch 15/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6948\n",
      "Epoch 453/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6941\n",
      "Epoch 495/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6947\n",
      "Epoch 16/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 454/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6941\n",
      "Epoch 496/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 17/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6942\n",
      "Epoch 455/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6947\n",
      "Epoch 497/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      "Epoch 18/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      "Epoch 456/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6936\n",
      "Epoch 498/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6935\n",
      "Epoch 19/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6944\n",
      "Epoch 457/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6935\n",
      "21/41 [==============>...............] - ETA: 0s - loss: 0.6934Epoch 499/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6937\n",
      "Epoch 20/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6936\n",
      "Epoch 458/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6938\n",
      "Epoch 500/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6934\n",
      " 6/41 [===>..........................] - ETA: 0s - loss: 0.6933Epoch 21/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6938\n",
      "Epoch 459/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6941\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6953\n",
      "Epoch 22/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6947\n",
      "Epoch 460/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 23/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 461/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6943\n",
      "Epoch 24/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 462/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 25/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 463/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6940\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 1s 4ms/steposs: 0.6925\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6936\n",
      "Epoch 464/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 27/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6949\n",
      "Epoch 465/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6932\n",
      "Epoch 28/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 466/500\n",
      "11/11 [==============================] - 0s 7ms/steposs: 0.6942\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 29/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6947\n",
      "Epoch 467/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 30/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 468/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 31/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 469/500\n",
      " 9/41 [=====>........................] - ETA: 0s - loss: 0.6918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6946\n",
      "Epoch 32/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 470/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 33/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6938Epoch 471/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 34/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 472/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6950\n",
      "Epoch 35/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6948\n",
      "Epoch 473/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 36/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6946\n",
      "Epoch 474/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 37/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6951\n",
      "Epoch 475/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6952\n",
      "Epoch 38/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      " 7/41 [====>.........................] - ETA: 0s - loss: 0.6905Epoch 476/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 39/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 477/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 40/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6955\n",
      "Epoch 478/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 41/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6947\n",
      "Epoch 479/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6942\n",
      "Epoch 42/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6935\n",
      "Epoch 480/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 43/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 481/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 44/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6949\n",
      "Epoch 482/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 45/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 483/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6944\n",
      "Epoch 46/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 484/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6942\n",
      "Epoch 47/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 485/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6945\n",
      "Epoch 48/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6946\n",
      "Epoch 486/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6943\n",
      "Epoch 49/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      "Epoch 487/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 50/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6949\n",
      "Epoch 488/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6945\n",
      "Epoch 51/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 489/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 52/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 490/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.6943Epoch 53/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 491/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 492/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 54/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6943\n",
      "Epoch 493/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 55/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6941\n",
      "Epoch 56/500\n",
      "Epoch 494/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 57/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6937\n",
      " 1/41 [..............................] - ETA: 2s - loss: 0.6927Epoch 495/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 58/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 496/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6961\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 59/500\n",
      "Epoch 497/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6942\n",
      "Epoch 60/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6927Epoch 498/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 61/500\n",
      "Epoch 499/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6944\n",
      "Epoch 500/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 62/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 63/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 64/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 0s 4ms/steposs: 0.69\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 0s 10ms/stepss: 0.693\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 67/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 68/500\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.6932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 69/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 70/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 71/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 72/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 73/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 74/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 75/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 76/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6948\n",
      "Epoch 77/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 78/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 79/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 80/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 81/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 82/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 83/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 84/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 85/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 86/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 87/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 88/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 89/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 90/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 91/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 92/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 93/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 94/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 95/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 96/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 97/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6948\n",
      "Epoch 98/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 99/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 100/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 101/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 102/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 103/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 104/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 105/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6955\n",
      "Epoch 106/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 107/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 108/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 109/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 110/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 111/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 112/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 113/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 114/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 115/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6949\n",
      "Epoch 116/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 117/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 118/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 119/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 120/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 121/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 122/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 123/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 124/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 125/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 126/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 127/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 128/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6948\n",
      "Epoch 129/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6945\n",
      "Epoch 130/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 131/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6949\n",
      "Epoch 132/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 133/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 134/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 135/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 136/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 137/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 138/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 139/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 140/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 141/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 142/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 143/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 144/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 145/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 146/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 147/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 148/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6937\n",
      "Epoch 149/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6943\n",
      "Epoch 150/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 151/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 152/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 153/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6958\n",
      "Epoch 154/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6940\n",
      "Epoch 155/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 156/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6949\n",
      "Epoch 157/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 158/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6948\n",
      "Epoch 159/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 160/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 161/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 162/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 163/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 164/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 165/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 166/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 167/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 168/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 169/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 170/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 171/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 172/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 173/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 174/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 175/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 176/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 177/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 178/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6953\n",
      "Epoch 179/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 180/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6939\n",
      "Epoch 181/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 182/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 183/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 184/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 185/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 186/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 187/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 188/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 189/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 190/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 191/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6950\n",
      "Epoch 192/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 193/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 194/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 195/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 196/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6947\n",
      "Epoch 197/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 198/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 199/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 200/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6957\n",
      "Epoch 201/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 202/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 203/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 204/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 205/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 206/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 207/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 208/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6932\n",
      "Epoch 209/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 210/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 211/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6945\n",
      "Epoch 212/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 213/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 214/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 215/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 216/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 217/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 218/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 219/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 220/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 221/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6941\n",
      "Epoch 222/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 223/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 224/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 225/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 226/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 227/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 228/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 229/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 230/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 231/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 232/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 233/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 234/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 235/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 236/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 237/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 238/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 239/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 240/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 241/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 242/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 243/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 244/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 245/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 246/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 247/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 248/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 249/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6952\n",
      "Epoch 250/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 251/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6946\n",
      "Epoch 252/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 253/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 254/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 255/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 256/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 257/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 258/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 259/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 260/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 261/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 262/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 263/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 264/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 265/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 266/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 267/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 268/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 269/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 270/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 271/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 272/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 273/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 274/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 275/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 276/500\n",
      "41/41 [==============================] - 0s 10ms/step - loss: 0.6940\n",
      "Epoch 277/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 278/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 279/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 280/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 281/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 282/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 283/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 284/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 285/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 286/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 287/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 288/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 289/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 290/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 291/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 292/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 293/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 294/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 295/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6952\n",
      "Epoch 296/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6947\n",
      "Epoch 297/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 298/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 299/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 300/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 301/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 302/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 303/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 304/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 305/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 306/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6950\n",
      "Epoch 307/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 308/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 309/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6939\n",
      "Epoch 310/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 311/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6947\n",
      "Epoch 312/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 313/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 314/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6952\n",
      "Epoch 315/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 316/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 317/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 318/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 319/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 320/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 321/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 322/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6944\n",
      "Epoch 323/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 324/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 325/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 326/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 327/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 328/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 329/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 330/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 331/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 332/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 333/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 334/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 335/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 336/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 337/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 338/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 339/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 340/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 341/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 342/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 343/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 344/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 345/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 346/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 347/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6952\n",
      "Epoch 348/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 349/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 350/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 351/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 352/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 353/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 354/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6958\n",
      "Epoch 355/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 356/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 357/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 358/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 359/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6934\n",
      "Epoch 360/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 361/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 362/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 363/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 364/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 365/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 366/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 367/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6944\n",
      "Epoch 368/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 369/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 370/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 371/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 372/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6951\n",
      "Epoch 373/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 374/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 375/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 376/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 377/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 378/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 379/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 380/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6946\n",
      "Epoch 381/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 382/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6946\n",
      "Epoch 383/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 384/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 385/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 386/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6933\n",
      "Epoch 387/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 388/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 389/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 390/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 391/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 392/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 393/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 394/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 395/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 396/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 397/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 398/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 399/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 400/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 401/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 402/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 403/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 404/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 405/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 406/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 407/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 408/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 409/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 410/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6938\n",
      "Epoch 411/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 412/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 413/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 414/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6945\n",
      "Epoch 415/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 416/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6937\n",
      "Epoch 417/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 418/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 419/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 420/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 421/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 422/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6938\n",
      "Epoch 423/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 424/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6944\n",
      "Epoch 425/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 426/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 427/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 428/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6934\n",
      "Epoch 429/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 430/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 431/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 432/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 433/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 434/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 435/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6940\n",
      "Epoch 436/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 437/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6940\n",
      "Epoch 438/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6933\n",
      "Epoch 439/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6933\n",
      "Epoch 440/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 441/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 442/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 443/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6937\n",
      "Epoch 444/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6939\n",
      "Epoch 445/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 446/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 447/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6935\n",
      "Epoch 448/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 449/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6942\n",
      "Epoch 450/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 451/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6948\n",
      "Epoch 452/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6952\n",
      "Epoch 453/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "Epoch 454/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 455/500\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 0.6936\n",
      "Epoch 456/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6941\n",
      "Epoch 457/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 458/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 459/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6945\n",
      "Epoch 460/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 461/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 462/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 463/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6935\n",
      "Epoch 464/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6951\n",
      "Epoch 465/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6936\n",
      "Epoch 466/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6943\n",
      "Epoch 467/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 468/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6932\n",
      "Epoch 469/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 470/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6942\n",
      "Epoch 471/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 472/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 473/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6941\n",
      "Epoch 474/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6945\n",
      "Epoch 475/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6939\n",
      "Epoch 476/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 477/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6946\n",
      "Epoch 478/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6955\n",
      "Epoch 479/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6941\n",
      "Epoch 480/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6938\n",
      "Epoch 481/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6959\n",
      "Epoch 482/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6938\n",
      "Epoch 483/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 484/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6936\n",
      "Epoch 485/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 486/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 487/500\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.6942\n",
      "Epoch 488/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 489/500\n",
      "41/41 [==============================] - 0s 12ms/step - loss: 0.6945\n",
      "Epoch 490/500\n",
      "41/41 [==============================] - 0s 11ms/step - loss: 0.6941\n",
      "Epoch 491/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6937\n",
      "Epoch 492/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6939\n",
      "Epoch 493/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6940\n",
      "Epoch 494/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6945\n",
      "Epoch 495/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 496/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6944\n",
      "Epoch 497/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6934\n",
      "Epoch 498/500\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.6936\n",
      "Epoch 499/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6943\n",
      "Epoch 500/500\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.6935\n",
      "11/11 [==============================] - 1s 4ms/step\n",
      "11/11 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(neural_net_wrapped,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logistic Regression + Random Forest\n",
    "\n",
    "The Neural Network did pretty bad as well! Based on the scores, looks like logistic regression and random forest are the 2 models worth tuning. Let's combine them to see what would happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;,\n",
       "                              LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                 penalty=None,\n",
       "                                                 random_state=42)),\n",
       "                             (&#x27;rf&#x27;,\n",
       "                              RandomForestClassifier(max_samples=0.7,\n",
       "                                                     n_estimators=1000,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))],\n",
       "                 n_jobs=-1, voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;,\n",
       "                              LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                 penalty=None,\n",
       "                                                 random_state=42)),\n",
       "                             (&#x27;rf&#x27;,\n",
       "                              RandomForestClassifier(max_samples=0.7,\n",
       "                                                     n_estimators=1000,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))],\n",
       "                 n_jobs=-1, voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, n_jobs=-1, penalty=None, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_samples=0.7, n_estimators=1000, n_jobs=-1,\n",
       "                       random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                 penalty=None,\n",
       "                                                 random_state=42)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(max_samples=0.7,\n",
       "                                                     n_estimators=1000,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "logistic_reg_stack = LogisticRegression(penalty=None,C=1.0,random_state=42,max_iter=1000,n_jobs=-1)\n",
    "random_forest_stack = RandomForestClassifier(n_estimators=1000,criterion='gini',bootstrap=True,n_jobs=-1,random_state=42,max_samples=0.7)\n",
    "\n",
    "# Stacked model\n",
    "log_forest = VotingClassifier(estimators=[('lr',logistic_reg_stack),('rf',random_forest_stack)],voting='soft',n_jobs=-1)\n",
    "\n",
    "# Fitting the model\n",
    "log_forest.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = log_forest.predict(features_preprocessed)\n",
    "train_probs = log_forest.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Ensemble 1 - Logistic Regression + Random Forest')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(log_forest,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(log_forest,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/log_forest_ensemble.sav','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logistic Regression + Random Forest (bit of regularization)\n",
    "\n",
    "Testing out the ensemble if both models have some regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;,\n",
       "                              LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                 penalty=None,\n",
       "                                                 random_state=42)),\n",
       "                             (&#x27;rf&#x27;,\n",
       "                              RandomForestClassifier(max_samples=0.7,\n",
       "                                                     n_estimators=1000,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))],\n",
       "                 n_jobs=-1, voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;,\n",
       "                              LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                 penalty=None,\n",
       "                                                 random_state=42)),\n",
       "                             (&#x27;rf&#x27;,\n",
       "                              RandomForestClassifier(max_samples=0.7,\n",
       "                                                     n_estimators=1000,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))],\n",
       "                 n_jobs=-1, voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, n_jobs=-1, penalty=None, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_samples=0.7, n_estimators=1000, n_jobs=-1,\n",
       "                       random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(max_iter=1000, n_jobs=-1,\n",
       "                                                 penalty=None,\n",
       "                                                 random_state=42)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(max_samples=0.7,\n",
       "                                                     n_estimators=1000,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "logistic_reg_stack_2 = LogisticRegression(penalty='elasticnet',C=0.3,random_state=42,max_iter=2000,n_jobs=-1,solver='saga',l1_ratio=0.6)\n",
    "random_forest_stack_2 = RandomForestClassifier(n_estimators=200,criterion='gini',bootstrap=True,n_jobs=-1,random_state=42,max_samples=0.7,\n",
    "                                             max_depth=7,min_samples_leaf=3,ccp_alpha=0.02)\n",
    "\n",
    "# Stacked model\n",
    "log_forest_2 = VotingClassifier(estimators=[('lr',logistic_reg_stack),('rf',random_forest_stack)],voting='soft',n_jobs=-1)\n",
    "\n",
    "# Fitting the model\n",
    "log_forest_2.fit(features_preprocessed,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training scores\n",
    "train_pred = log_forest_2.predict(features_preprocessed)\n",
    "train_probs = log_forest_2.predict_proba(features_preprocessed)[:,1]\n",
    "\n",
    "# Getting the metrics\n",
    "train_brier = brier_score_loss(target,train_probs)\n",
    "train_accuracy = accuracy_score(target,train_pred)\n",
    "\n",
    "# Adding metrics to the dictionary\n",
    "model_list = metrics['Model']\n",
    "model_list.append('Ensemble 1 - Logistic Regression + Random Forest with Regularization')\n",
    "metrics['Model'] = model_list\n",
    "train_briers = metrics['Brier Score Training']\n",
    "train_briers.append(train_brier)\n",
    "metrics['Brier Score Training'] = train_briers\n",
    "train_accuracies = metrics['Accuracy Training']\n",
    "train_accuracies.append(train_accuracy)\n",
    "metrics['Accuracy Training'] = train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results and adding it to the dictionary\n",
    "results = cross_validate(log_forest_2,features_preprocessed,target,scoring=['accuracy','neg_brier_score'],\n",
    "                         cv=5,n_jobs=-1)\n",
    "\n",
    "cv_briers = metrics['Mean Brier Score CV']\n",
    "cv_briers.append((-1*results['test_neg_brier_score']).mean())\n",
    "metrics['Mean Brier Score CV'] = cv_briers\n",
    "cv_accuracies = metrics['Mean Accuracy CV']\n",
    "cv_accuracies.append((results['test_accuracy']).mean())\n",
    "metrics['Mean Accuracy CV'] = cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in a pickle file\n",
    "pickle.dump(log_forest_2,open('/Users/jinalshah/Jinal/Projects/march-madness-mania/models/log_forest_ensemble_2.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Brier Score Training</th>\n",
       "      <th>Mean Brier Score CV</th>\n",
       "      <th>Accuracy Training</th>\n",
       "      <th>Mean Accuracy CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.51681</td>\n",
       "      <td>0.50124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression - No Regularization</td>\n",
       "      <td>0.16506</td>\n",
       "      <td>0.22291</td>\n",
       "      <td>0.76526</td>\n",
       "      <td>0.66005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree - UnRegularized</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.41093</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.58907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest - Basic</td>\n",
       "      <td>0.05107</td>\n",
       "      <td>0.20731</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.67310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adaboost - Logistic Regression</td>\n",
       "      <td>0.24962</td>\n",
       "      <td>0.24968</td>\n",
       "      <td>0.74533</td>\n",
       "      <td>0.66504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adaboost - Decision Tree</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42714</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.57286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elastic Net - C = 1 and p = 0.7</td>\n",
       "      <td>0.16821</td>\n",
       "      <td>0.21252</td>\n",
       "      <td>0.74658</td>\n",
       "      <td>0.67562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Catboost - Gradient Boosting</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.26023</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.65065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Deep Neural Network - No Dropout</td>\n",
       "      <td>0.49626</td>\n",
       "      <td>0.49626</td>\n",
       "      <td>0.50374</td>\n",
       "      <td>0.50374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ensemble 1 - Logistic Regression + Random Forest</td>\n",
       "      <td>0.09562</td>\n",
       "      <td>0.20315</td>\n",
       "      <td>0.91407</td>\n",
       "      <td>0.67686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ensemble 1 - Logistic Regression + Random Fore...</td>\n",
       "      <td>0.09562</td>\n",
       "      <td>0.20315</td>\n",
       "      <td>0.91407</td>\n",
       "      <td>0.67686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model  Brier Score Training  \\\n",
       "0                                    Dummy Classifier               0.25000   \n",
       "1             Logistic Regression - No Regularization               0.16506   \n",
       "2                       Decision Tree - UnRegularized               0.00000   \n",
       "3                               Random Forest - Basic               0.05107   \n",
       "4                      Adaboost - Logistic Regression               0.24962   \n",
       "5                            Adaboost - Decision Tree               0.00000   \n",
       "6                     Elastic Net - C = 1 and p = 0.7               0.16821   \n",
       "7                        Catboost - Gradient Boosting               0.00002   \n",
       "8                    Deep Neural Network - No Dropout               0.49626   \n",
       "9    Ensemble 1 - Logistic Regression + Random Forest               0.09562   \n",
       "10  Ensemble 1 - Logistic Regression + Random Fore...               0.09562   \n",
       "\n",
       "    Mean Brier Score CV  Accuracy Training  Mean Accuracy CV  \n",
       "0               0.25000            0.51681           0.50124  \n",
       "1               0.22291            0.76526           0.66005  \n",
       "2               0.41093            1.00000           0.58907  \n",
       "3               0.20731            1.00000           0.67310  \n",
       "4               0.24968            0.74533           0.66504  \n",
       "5               0.42714            1.00000           0.57286  \n",
       "6               0.21252            0.74658           0.67562  \n",
       "7               0.26023            1.00000           0.65065  \n",
       "8               0.49626            0.50374           0.50374  \n",
       "9               0.20315            0.91407           0.67686  \n",
       "10              0.20315            0.91407           0.67686  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting metrics into a dataframe\n",
    "pd.options.display.float_format = '{:.5f}'.format # Making sure it doesn't display in scientific notation\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results to view\n",
    "metrics_df.to_csv('/Users/jinalshah/Jinal/Projects/march-madness-mania/results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "march-madness-mania",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd35a392fde1dc673d11d63530ba2243a9ec2532d473e3b8a20605e035add6e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
